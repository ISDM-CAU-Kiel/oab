{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100a6f8d",
   "metadata": {
    "id": "100a6f8d"
   },
   "source": [
    "\n",
    "## In this notebook, you will see all the steps sequentially performed to be able to utilize the complete functionality of OAB framework. The steps are as follows :\n",
    "0. SETUP\n",
    "1. DATA\n",
    "2. DATA SELECTION\n",
    "3. PREPROCESSING\n",
    "4. SAMPLING\n",
    "5. ALGORITHM TRAINING AND TESTING\n",
    "6. EVALUATION\n",
    "7. SHOW BENCHMARK RESULTS\n",
    "8. REPRODUCIBILTY\n",
    "9. EXTENDING THE BENCHMARK(with own Algorithm)\n",
    "\n",
    "This notebook focuses on <b>Semisupervised Tabular Data</b>. Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VXCXUMi0bRJA",
   "metadata": {
    "id": "VXCXUMi0bRJA"
   },
   "source": [
    "# **0. SETUP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kkhqUcFybZ56",
   "metadata": {
    "id": "kkhqUcFybZ56"
   },
   "source": [
    "`oab` framework can be integrated in your Python environment  as a PyPi package  using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9rc17bsZbbnb",
   "metadata": {
    "id": "9rc17bsZbbnb"
   },
   "outputs": [],
   "source": [
    "#ID 1(0)\n",
    "\n",
    "#%%capture\n",
    "# pip install oab\n",
    "!pip install example-pkg-jd-kiel --extra-index-url=https://test.pypi.org/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceda5c4",
   "metadata": {},
   "source": [
    "`Cloning` the repository:\n",
    "\n",
    "oab is an open-source framework which can be accessed at https://github.com/ISDM-CAU-Kiel/oab. To use this .ipynb notebook successfully, the formerly mentioned repository needs to be cloned with the following command and this notebook must be run(if this is not the case already) within the cloned repository from the path:\n",
    "\n",
    "<b>/oab/Tutorials/Semisupervised_Anomaly_Detection_on_Benchmark_Tabular_Data.ipynb</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID 2(0)\n",
    "!git clone https://github.com/ISDM-CAU-Kiel/oab.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821d1f1",
   "metadata": {},
   "source": [
    "Now, importing the necessary functions and internal variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db36b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 3(0)\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime \n",
    "from pathlib import Path\n",
    "#sys.path.append('../..')           \n",
    "sys.path.insert(0,f\"{Path(os.getcwd()).parent}\") # setting the parent directory of repository as current directory\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# necessary imports for loading datasets as well as information from recipe files\n",
    "\n",
    "from oab.data.semisupervised import SemisupervisedAnomalyDataset\n",
    "from oab.data.load_dataset import load_dataset\n",
    "\n",
    "from oab.data.load_recipe_functions import *\n",
    "# necessary imports for algorithm comparisons and defining seeds\n",
    "from oab.evaluation import EvaluationObject, ComparisonObject,all_metrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb70c2",
   "metadata": {
    "id": "50eb70c2"
   },
   "source": [
    "## **0.1 NOTEBOOK AND CELL STRUCTURE** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4cbba",
   "metadata": {
    "id": "bca4cbba"
   },
   "source": [
    "In this notebook there are certain sections where the user s required to enter its own information which are marked as comments of the form :\n",
    "\n",
    "<b>### ADD YOUR CODE ###</b>  , so <b>###</b> can be searched to know what are those sections.\n",
    "\n",
    "All cells are assigned an ID, as a comment at the top of the cell,for example as: <b>#ID 10(5)</b>, where 10 denotes the cell ID and 5 denotes the Sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25722a3f",
   "metadata": {},
   "source": [
    "## 0.2 DETAILS OF THIS BENCHMARK RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9440db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211217000524-Paper_A-sst-recipe.yaml\n"
     ]
    }
   ],
   "source": [
    "#ID 4(0)\n",
    "### ADD YOUR BENCHMARK NAME HERE ###\n",
    "benchmark_name=\"Paper_A\" \n",
    "\n",
    "\n",
    "datasets_info_path=Path(os.getcwd()).parent/\"oab\"/\"data\"/\"datasets.yaml\" # getting path to \"datasets.yaml\" which contains information about all tabular datasets\n",
    "\n",
    "recipes_parent_path=Path(os.getcwd()).parent/\"notebooks\"/\"benchmark_tabular\"\n",
    "dataset_folder=recipes_parent_path/\"datasets\" # all dataset-folders are contained in this folder\n",
    "#print(dataset_folder)\n",
    "\n",
    "benchmark_type=\"sst\"     # benchmark run for semisupervised tabular datasets(sst)\n",
    "if not os.path.exists(recipes_parent_path/benchmark_name): #creating directory for this benchhmark for storing recipes\n",
    "    os.makedirs(recipes_parent_path/benchmark_name)\n",
    "\n",
    "    \n",
    "time=datetime.now().strftime(\"%Y%m%d%H%M%S\") # timestamp set for this run  \n",
    "new_recipe_path=f\"{recipes_parent_path}/{benchmark_name}/{time}-{benchmark_name}-{benchmark_type}-recipe.yaml\" # recipe path for new recipe created in this run   \n",
    "print(f\"{time}-{benchmark_name}-{benchmark_type}-recipe.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083fa64",
   "metadata": {},
   "source": [
    "### For reproducing a previously-created recipe without adding new datasets and algorithms from this benchmark  ,  skip to :\n",
    "\n",
    "### `#ID 20(5)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ge7C0gfOXtkJ",
   "metadata": {
    "id": "Ge7C0gfOXtkJ"
   },
   "source": [
    "# **1. DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714d531",
   "metadata": {
    "id": "7714d531"
   },
   "source": [
    "First of all, we will have a look at the Datasets that are pre-installed in OAB which can be used for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c45b98e",
   "metadata": {
    "id": "1c45b98e",
    "outputId": "de6c3c28-b6c3-4dd5-cf4d-c9dbc018a5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.page-blocks\n",
      "1.spambase\n",
      "2.wilt\n",
      "3.pulsar_star\n",
      "4.forest_cover\n",
      "5.NASA_ground_data\n",
      "6.wine\n",
      "7.boston\n",
      "8.http\n",
      "9.smtp\n",
      "10.cardio\n",
      "11.thyroid\n",
      "12.musk\n",
      "13.pima\n",
      "14.shuttle\n",
      "15.breastw\n",
      "16.arrhytmia\n",
      "17.ionosphere\n",
      "18.optdigits\n",
      "19.mammography\n",
      "20.annthyroid\n",
      "21.pendigits\n",
      "22.vertebral\n"
     ]
    }
   ],
   "source": [
    "# ID 5(1)\n",
    "for i in get_tabular_dataset_names():\n",
    "    print(f\"{i[0]}.{i[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5GfTylvidJUs",
   "metadata": {
    "id": "5GfTylvidJUs"
   },
   "source": [
    "\n",
    "`oab` provides a variety of tabular datasets that can easily be loaded, \n",
    "\n",
    "`1.` If a user is interested in using her own tabular dataset, the following steps have to be followed: \n",
    "\n",
    " (a) Ensure that `own` dataset(s) information is stored in the file `datasets.yaml` which is located at\n",
    "<b>\"/oab/data\"</b>  (by executon of  #ID 7(1))\n",
    " \n",
    " \n",
    " (b) The `folder` containing the dataset must be stored in `datasets` folder of the OAB and name should be same as the dataset_name\n",
    " \n",
    " (c) Then, own dataset(s) can be loaded just like the pre-installed OAB datasets \n",
    " \n",
    " `2.` If user's dataset is provided **via a URL**, then it would be downloaded and stored in the OAB's \"datasets\" folder.\n",
    " \n",
    " The files in the `folder` that are downloaded or stored manually in \"datasets\" folder , can be of variety of formats such as:\n",
    " \n",
    " 1. 'csv           \n",
    " 2. 'zip'\n",
    " 3.'gz_single_file'\n",
    " 4.'mat'\n",
    " 5.'mat_old\n",
    " \n",
    " If user has her dataset file in one of these formats, or has multiple files, then `oab` automatically makes one file out of that which is then input to the oab. We will see the case here, when user loads her dataset through local `folder directory`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b9a67",
   "metadata": {},
   "source": [
    "Here's the structure of how the datasets' information are stored in `datasets.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff2c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page-blocks:\r\n",
      "  name: \"page-blocks\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"page-blocks\"\r\n",
      "  urls_dataset: [\"https://pkgstore.datahub.io/machine-learning/page-blocks/page-blocks_csv/data/7c1adeffd3ce22181986879d92f9508c/page-blocks_csv.csv\"]\r\n",
      "  destination_filenames: [\"page_blocks.csv\"]\r\n",
      "  dataset_format: 'csv'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"page_blocks.csv\" # Note: If we have multiple files, this should still be just one!\r\n",
      "  load_csv_arguments:\r\n",
      "    header: 0\r\n",
      "  class_labels: \"last\" # or \"first\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test1.yaml\" # for preprocessing and making an anomaly dataset\r\n",
      "  destination_yaml: \"page-blocks_preprocessing.yaml\"\r\n",
      "  credits: \"Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. For more information, check https://archive.ics.uci.edu/ml/datasets/Page+Blocks+Classification.\"\r\n",
      "  notes: \"Original data is in .Z format, and unpacking this in Python without OS depencies is problematic. Therefore, the dataset is loaded from a different source.\"\r\n",
      "\r\n",
      "spambase:\r\n",
      "  name: \"spambase\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"spambase\"\r\n",
      "  urls_dataset: [\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"]\r\n",
      "  destination_filenames: [\"spambase.csv\"]\r\n",
      "  dataset_format: 'csv'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"spambase.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test2.yaml\"\r\n",
      "  destination_yaml: \"spambase_preprocessing.yaml\"\r\n",
      "  credits: \"Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\"\r\n",
      "\r\n",
      "wilt:\r\n",
      "  name: \"wilt\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"wilt\"\r\n",
      "  urls_dataset: [\"https://archive.ics.uci.edu/ml/machine-learning-databases/00285/wilt.zip\"]\r\n",
      "  destination_filenames: [\"wilt.zip\"]\r\n",
      "  dataset_format: 'zip'\r\n",
      "  filenames_to_concatenate: [\"training.csv\", \"testing.csv\"]\r\n",
      "  filename_in_folder: \"wilt.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: 0\r\n",
      "  class_labels: \"first\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test3.yaml\"\r\n",
      "  destination_yaml: \"wilt_preprocessing.yaml\"\r\n",
      "  credits: \"Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\"\r\n",
      "\r\n",
      "pulsar_star:\r\n",
      "  name: \"pulsar_star\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"pulsar_star\"\r\n",
      "  urls_dataset: [\"https://archive.ics.uci.edu/ml/machine-learning-databases/00372/HTRU2.zip\"]\r\n",
      "  destination_filenames: [\"HTRU2.zip\"]\r\n",
      "  dataset_format: 'zip'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"HTRU_2.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test4.yaml\"\r\n",
      "  destination_yaml: \"pulsar_star_preprocessing.yaml\"\r\n",
      "  credits: \"Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\"\r\n",
      "\r\n",
      "forest_cover:\r\n",
      "  name: \"forest_cover\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"forest_cover\"\r\n",
      "  urls_dataset: [\"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"]\r\n",
      "  destination_filenames: [\"forest_cover.data.gz\"]\r\n",
      "  dataset_format: 'gz_single_file' # note: this is not a folder that is compressed, but a single file\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"forest_cover.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test5.yaml\"\r\n",
      "  destination_yaml: \"forest_cover_preprocessing.yaml\"\r\n",
      "  credits: \"Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\"\r\n",
      "\r\n",
      "NASA_ground_data:\r\n",
      "  name: \"NASA_ground_data\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"NASA_ground_data\"\r\n",
      "  urls_dataset: [\"https://www.openml.org/data/get_csv/53950\"]\r\n",
      "  destination_filenames: [\"NASA_ground_data.csv\"]\r\n",
      "  dataset_format: 'csv'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"NASA_ground_data.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: 0\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test6.yaml\"\r\n",
      "  destination_yaml: \"NASA_ground_data_preprocessing.yaml\"\r\n",
      "  credits: \"Sayyad Shirabad, J. and Menzies, T.J. (2005) The PROMISE Repository of Software Engineering Databases. School of Information Technology and Engineering, University of Ottawa, Canada.\"\r\n",
      "\r\n",
      "wine:\r\n",
      "  name: \"wine\"\r\n",
      "  short_description: \"In accordance with http://odds.cs.stonybrook.edu/wine-dataset/, labels 2 and 3 are normal, label 1 is anomalous.\"\r\n",
      "  foldername: \"wine\"\r\n",
      "  urls_dataset: [\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"]\r\n",
      "  destination_filenames: [\"wine.csv\"]\r\n",
      "  dataset_format: 'csv'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"wine.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"first\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test7.yaml\"\r\n",
      "  destination_yaml: \"wine_preprocessing.yaml\"\r\n",
      "  credits: \"Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\"\r\n",
      "\r\n",
      "boston:\r\n",
      "  name: \"boston\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"boston\"\r\n",
      "  urls_dataset: [\"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"]\r\n",
      "  destination_filenames: [\"boston.csv\"]\r\n",
      "  dataset_format: 'csv'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"boston.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: 0\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test9.yaml\"\r\n",
      "  destination_yaml: \"boston_preprocessing.yaml\"\r\n",
      "  credits: \"https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\"\r\n",
      "\r\n",
      "http:\r\n",
      "  name: \"http\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"http\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/iy9ucsifal754tp/http.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"http.mat\"]\r\n",
      "  dataset_format: 'mat'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"http.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"http_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/http-kddcup99-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "smtp:\r\n",
      "  name: \"smtp\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"smtp\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/dbv2u4830xri7og/smtp.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"smtp.mat\"]\r\n",
      "  dataset_format: 'mat'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"smtp.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"smtp_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/smtp-kddcup99-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "cardio:\r\n",
      "  name: \"cardio\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"cardio\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/galg3ihvxklf0qi/cardio.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"cardio.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"cardio.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"cardio_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/cardiotocogrpahy-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "thyroid:\r\n",
      "  name: \"thyroid\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"thyroid\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/bih0e15a0fukftb/thyroid.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"thyroid.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"thyroid.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"thyroid_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/thyroid-disease-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "musk:\r\n",
      "  name: \"musk\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"musk\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/we6aqhb0m38i60t/musk.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"musk.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"musk.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"musk_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/musk-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "pima: # no longer exists http://odds.cs.stonybrook.edu/pima-indians-diabetes-dataset/\r\n",
      "  name: \"pima\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"pima\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/mvlwu7p0nyk2a2r/pima.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"pima.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"pima.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"pima_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/pima-indians-diabetes-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "shuttle:\r\n",
      "  name: \"shuttle\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"shuttle\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/mk8ozgisimfn3dw/shuttle.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"shuttle.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"shuttle.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"shuttle_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/shuttle-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "breastw:\r\n",
      "  name: \"breastw\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"breastw\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/g3hlnucj71kfvq4/breastw.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"breastw.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"breastw.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"breastw_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/breast-cancer-wisconsin-original-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "arrhytmia:\r\n",
      "  name: \"arrhytmia\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"arrhytmia\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/lmlwuspn1sey48r/arrhythmia.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"arrhytmia.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"arrhytmia.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"arrhytmia_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/arrhytmia-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "ionosphere:\r\n",
      "  name: \"ionosphere\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"ionosphere\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/lpn4z73fico4uup/ionosphere.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"ionosphere.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"ionosphere.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"ionosphere_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/ionosphere-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "optdigits:\r\n",
      "  name: \"optdigits\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"optdigits\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/w52ndgz5k75s514/optdigits.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"optdigits.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"optdigits.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"optdigits_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/optdigits-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "mammography:\r\n",
      "  name: \"mammography\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"mammography\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/tq2v4hhwyv17hlk/mammography.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"mammography.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"mammography.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"mammography_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/mammography-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "annthyroid:\r\n",
      "  name: \"annthyroid\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"annthyroid\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/aifk51owxbogwav/annthyroid.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"annthyroid.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"annthyroid.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"annthyroid_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/annthyroid-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "pendigits:\r\n",
      "  name: \"pendigits\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"pendigits\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/1x8rzb4a0lia6t1/pendigits.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"pendigits.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"pendigits.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"pendigits_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/pendigits-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n",
      "\r\n",
      "vertebral:\r\n",
      "  name: \"vertebral\"\r\n",
      "  short_description:\r\n",
      "  foldername: \"vertebral\"\r\n",
      "  urls_dataset: [\"https://www.dropbox.com/s/5kuqb387sgvwmrb/vertebral.mat?dl=1\"]\r\n",
      "  destination_filenames: [\"vertebral.mat\"]\r\n",
      "  dataset_format: 'mat_old'\r\n",
      "  filenames_to_concatenate: null\r\n",
      "  filename_in_folder: \"vertebral.csv\"\r\n",
      "  load_csv_arguments:\r\n",
      "    header: null\r\n",
      "  class_labels: \"last\"\r\n",
      "  url_yaml: \"https://raw.githubusercontent.com/jandeller/test/main/test13.yaml\"\r\n",
      "  destination_yaml: \"vertebral_preprocessing.yaml\"\r\n",
      "  credits: \"Dataset provided by OODS on http://odds.cs.stonybrook.edu/vertebral-dataset/. If you use this for publications, please keep the publication policy in mind (http://odds.cs.stonybrook.edu/about-odds/). \\n Shebuti Rayana (2016).  ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\"\r\n"
     ]
    }
   ],
   "source": [
    "# ID 6(1)\n",
    "!cat {datasets_info_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Jap0jcY8JQwb",
   "metadata": {
    "id": "Jap0jcY8JQwb",
    "outputId": "56d80fce-d7ea-4693-aa71-41e58b766d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'myTabularDataset': <oab.data.classification_dataset.ClassificationDataset object at 0x7f4483c790a0>}\n"
     ]
    }
   ],
   "source": [
    "#ID 7(1)\n",
    "\n",
    "\n",
    "### ADD OWN DATASET(S) DETAILS ###   see #ID 3(1) for exact parameters which are to be entered\n",
    "\n",
    "own_datasets_info= [{\n",
    "                        'dataset_name':'myTabularDataset',\n",
    "                    }]\n",
    "\n",
    "\n",
    " # 'myTabularDataset' is the name of the Dataset(which the user loads for benchmarking)\n",
    "### Add more dictionaries to the list `own_datasets_info` with datasets information like example below\n",
    "                   #           { \n",
    "                   #               'dataset_name':'XYZDataset'\n",
    "                   #               'filenames_to_concatenate':['train_example.csv','test_example.csv']\n",
    "                   #               ....\n",
    "                   #            }\n",
    "                \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "### ADD DATASETNAME(S) FROM OAB'S DATASETS HERE ###\n",
    "\n",
    "benchmark_datasets_list=['spambase']   # More of OAB's datasets can be added to this list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## contains dataset objects of own_datasets_list as well as benchmark_datasets_list   \n",
    "datasets={}\n",
    "\n",
    "\n",
    "# Adding and Loading own datasets\n",
    "for dataset_details in own_datasets_info:\n",
    "     datasets[dataset_details['dataset_name']]=load_own_tabular_dataset(**dataset_details)\n",
    "\n",
    "\n",
    "print(datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c1bdac",
   "metadata": {
    "id": "c0c1bdac"
   },
   "source": [
    "# **2. DATA SELECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NZ9L0Cc-NPsB",
   "metadata": {
    "id": "NZ9L0Cc-NPsB"
   },
   "source": [
    "\n",
    "Datasets can either be loaded directly as anomaly datasets or as classification datasets. In the former case, the dataset is automatically fully prepared and ready for sampling. In the latter case, further preprocessing is still possible and necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11dpL-kh5qX",
   "metadata": {
    "id": "f11dpL-kh5qX"
   },
   "source": [
    "**After adding and loading own dataset(s) in #ID 7(1) and now the user is able to select other benchmarking datasets:**\n",
    "\n",
    "Now,  we'll have a look at all the datasets again which are pre-installed in OAB, so that they can be chosen for the benchmark run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "BkZNi850qMy6",
   "metadata": {
    "id": "BkZNi850qMy6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.NASA_ground_data\n",
      "1.annthyroid\n",
      "2.arrhytmia\n",
      "3.boston\n",
      "4.breastw\n",
      "5.cardio\n",
      "6.forest_cover\n",
      "7.http\n",
      "8.ionosphere\n",
      "9.mammography\n",
      "10.musk\n",
      "11.myTabularDataset\n",
      "12.optdigits\n",
      "13.page-blocks\n",
      "14.pendigits\n",
      "15.pima\n",
      "16.pulsar_star\n",
      "17.shuttle\n",
      "18.smtp\n",
      "19.spambase\n",
      "20.thyroid\n",
      "21.vertebral\n",
      "22.wilt\n",
      "23.wine\n"
     ]
    }
   ],
   "source": [
    "#ID 8(2)\n",
    "\n",
    "\n",
    "for i in get_tabular_dataset_names():\n",
    "    print(f\"{i[0]}.{i[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jZrY4gxINSqv",
   "metadata": {
    "id": "jZrY4gxINSqv"
   },
   "source": [
    "### **2.1 Load anomaly detection datasets (with or without further preprocessing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1GoNZY8YNSu0",
   "metadata": {
    "id": "1GoNZY8YNSu0"
   },
   "source": [
    "In this section, we load some pre-installed data sets. This can be achieved using the `load_dataset` function. By default, it creates an anomaly dataset from which sampling is directly possible  but we can first create classsifcation dataset and then anomaly dataset,either with the preprocessing applied (`preprocess_classification_dataset=True`) i.e. standard or custom operations like treat_missing_values,delete_columns,etc. are performed, or without (`preprocess_classification_dataset=False`, default).\n",
    "\n",
    "`In our case` we set have already imported own datasets with `anomaly_dataset=False ` and `preprocess_classification_dataset=False` in <b>#ID 7(1)</b> and we will also load the OAB datasets in the same way in <b>#ID 9(2)</b>\n",
    "\n",
    "Note that as discussed in the paper, multiclass classification datasets like `spambase` and `annthyroid` are loaded with the class label `0` as normal label and all other labels as anomaly labels by default. (Alternatively, `oab` can automatically iterate through all classes as normal classes. This is not covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6VrjgVJANX8V",
   "metadata": {
    "id": "6VrjgVJANX8V",
    "outputId": "2608f64a-9804-41e0-e8be-5656206565d9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credits: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n"
     ]
    }
   ],
   "source": [
    "#ID 9(2)\n",
    "\n",
    "#### ADD YOUR OWN NUMBER OF DATASETS AND FROM OAB FOR BENCHMARKING  ###\n",
    "\n",
    "\n",
    "for dataset_name in benchmark_datasets_list:  # loading benchmark's datasets\n",
    "    datasets[dataset_name]=load_dataset(dataset_name,anomaly_dataset=False,preprocess_classification_dataset=False,dataset_folder=dataset_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d6a779b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'myTabularDataset': <oab.data.classification_dataset.ClassificationDataset object at 0x7f4483c790a0>, 'spambase': <oab.data.classification_dataset.ClassificationDataset object at 0x7f4483b72f40>}\n"
     ]
    }
   ],
   "source": [
    "#ID 10(2)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad12196",
   "metadata": {
    "id": "7ad12196"
   },
   "source": [
    "# **3. PREPROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Balwbv2ANnT-",
   "metadata": {
    "id": "Balwbv2ANnT-"
   },
   "source": [
    "Standard preprocessing steps(or Custom preprocessing steps which are defined by user) like deleting columns, encoding categorical values differently, or removing missing values can be performed  to tabular data. Therefore, these methods (as well as own preprocessing steps and how these are captured) are covered here in this section.\n",
    "\n",
    "Here, we only show two preprocessing steps that are applied to datasets in `preprocess_datasets`(loaded in 2.2), which can also be performed individually depending upon requirement :\n",
    "- Perform `Standard/Custom Preprocessing functions`\n",
    "- `Transform the dataset into an anomaly dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "FhPX8wPfNmwF",
   "metadata": {
    "id": "FhPX8wPfNmwF",
    "outputId": "ec16404b-0d29-4bae-e5c9-e4ee5e438d0d"
   },
   "outputs": [],
   "source": [
    "#ID 11(3)                            SCALING APPLIED\n",
    "\n",
    "#used imports from #ID 2(1)                                          \n",
    "for dataset_name in datasets:\n",
    "    \n",
    "    \n",
    "    datasets[dataset_name].treat_missing_values()\n",
    "    datasets[dataset_name].normalize_columns()\n",
    "    datasets[dataset_name].delete_duplicates()\n",
    "    operations=datasets[dataset_name].operations_performed\n",
    "    dataset_info_store(dataset_name,new_recipe_path,info_type='standard_functions',content=operations) \n",
    "   \n",
    "\n",
    "\n",
    "#print(\"preprocesing performed on datasets! \")    \n",
    "#print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IZUpLqmtsyGc",
   "metadata": {
    "id": "IZUpLqmtsyGc"
   },
   "source": [
    "The file <b>f\"{time}-{benchmark_name}-{benchmark_type}-recipe.yaml\",</b> now contains information about how to preprocess(i.e. perform scaling) the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "DWm4l0mUstgE",
   "metadata": {
    "id": "DWm4l0mUstgE",
    "outputId": "5519f8cf-7f05-4a88-8c53-75382ee30475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myTabularDataset:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "spambase:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n"
     ]
    }
   ],
   "source": [
    "#ID 12(3)\n",
    "\n",
    "!cat {new_recipe_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ZSTGiI5Qsi6e",
   "metadata": {
    "id": "ZSTGiI5Qsi6e",
    "outputId": "9ea47a01-a842-44ff-d192-6cd9e1627b16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets after adding anomaly-conversion datasets: \n",
      "{'myTabularDataset': <oab.data.semisupervised.SemisupervisedAnomalyDataset object at 0x7f4488b646d0>, 'spambase': <oab.data.semisupervised.SemisupervisedAnomalyDataset object at 0x7f4483b96f40>}\n"
     ]
    }
   ],
   "source": [
    "#ID 13(3)                            ANOMALY-DATASET CONVERSION PERFORMED\n",
    "\n",
    "#used import from #ID 2 \n",
    "\n",
    "\n",
    "#recipe_path=f\"{benchmark_name}/{time}_{benchmark_name}_recipe.yaml\"                                           \n",
    "\n",
    "datasets_ad={}    \n",
    "    # for storing dataset objects converted to anomaly-dataset\n",
    "for dataset_name in datasets:   \n",
    "    \n",
    "     datasets_ad[dataset_name]= SemisupervisedAnomalyDataset(classification_dataset=datasets[dataset_name],\n",
    "                                                       normal_labels=0)  \n",
    "   \n",
    "     normal_labels=datasets_ad[dataset_name].normal_labels \n",
    "     dataset_info_store(dataset_name,new_recipe_path,info_type='anomaly_dataset',content=normal_labels)   \n",
    "                                                                            \n",
    "print(\"datasets after adding anomaly-conversion datasets: \")    \n",
    "print(datasets_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4HSf5lCJczbu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HSf5lCJczbu",
    "outputId": "8573e270-eb3c-4c2e-e399-084b104a21b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myTabularDataset:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels:\r\n",
      "      - 0\r\n",
      "      anomaly_labels:\r\n",
      "spambase:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels:\r\n",
      "      - 0\r\n",
      "      anomaly_labels:\r\n"
     ]
    }
   ],
   "source": [
    "#ID 14(3)\n",
    "\n",
    "!cat {new_recipe_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f600e5a",
   "metadata": {
    "id": "4f600e5a"
   },
   "source": [
    "# **4. SAMPLING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f817a6ab",
   "metadata": {
    "id": "f817a6ab"
   },
   "source": [
    "Here, we define the sampling parameters to sample from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bae62b9",
   "metadata": {
    "id": "6bae62b9",
    "outputId": "eb1b0b03-efa8-46d7-99fd-34dc771313e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'semisupervised_training_split_multiple': {'training_split': 0.6, 'max_contamination_rate': 0.4, 'n_steps': 4}}]\n"
     ]
    }
   ],
   "source": [
    "#ID 15(4)\n",
    "\n",
    "### ADD YOUR OWN SAMPLING PARAMETERS ###\n",
    "\n",
    "# sampling parameters\n",
    "\n",
    "# sampling parameters\n",
    "training_split = 0.6                 # specifies the proportion of normal data points that will be used during training\n",
    "max_contamination_rate = 0.4        # Maximum contamination rate of the test set. If this is exceeded, not all anomalies that exist are sampled \n",
    "n_steps = 4        # n_steps=10      # Number of samples to be taken\n",
    "\n",
    "\n",
    "#These below are the possible sampling types to sample from datasets\n",
    "sampling_types=['semisupervised_multiple','semisupervised_explicit_numbers_single','semisupervised_training_split_multiple','semisupervised_training_split_single']\n",
    "\n",
    "sampling_type='semisupervised_training_split_multiple'  #by default for this run\n",
    "\n",
    "sampling_params_current_run=[{'training_split':training_split,'max_contamination_rate':max_contamination_rate,'n_steps':n_steps},sampling_type] \n",
    "\n",
    "sampling=[{sampling_type:sampling_params_current_run[0]}]\n",
    "print(sampling)\n",
    "\n",
    "#storing sampling info to recipe\n",
    "for dataset_name in datasets_ad:\n",
    "    dataset_info_store(dataset_name,new_recipe_path,'sampling',content=sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZpZe-6r6tdIl",
   "metadata": {
    "id": "ZpZe-6r6tdIl"
   },
   "source": [
    "The above sampling parameters are utilized in\n",
    "<b>#ID 23(5)</b> \n",
    "for sampling the datasets before training the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d171e56",
   "metadata": {
    "id": "2d171e56",
    "outputId": "82d3de15-5fbc-438b-f7ae-9398b17ec140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myTabularDataset:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels:\r\n",
      "      - 0\r\n",
      "      anomaly_labels:\r\n",
      "- sampling:\r\n",
      "    semisupervised_training_split_multiple:\r\n",
      "      training_split: 0.6\r\n",
      "      max_contamination_rate: 0.4\r\n",
      "      n_steps: 4\r\n",
      "spambase:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels:\r\n",
      "      - 0\r\n",
      "      anomaly_labels:\r\n",
      "- sampling:\r\n",
      "    semisupervised_training_split_multiple:\r\n",
      "      training_split: 0.6\r\n",
      "      max_contamination_rate: 0.4\r\n",
      "      n_steps: 4\r\n"
     ]
    }
   ],
   "source": [
    "# ID 16(4)\n",
    "!cat {new_recipe_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bbba70",
   "metadata": {
    "id": "73bbba70"
   },
   "source": [
    "Now, we will associate sampling information with each dataset loaded in the benchmark run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a49db16",
   "metadata": {
    "id": "3a49db16",
    "outputId": "a83a0e73-9603-4931-9bc3-542c2931d517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'myTabularDataset': [<oab.data.semisupervised.SemisupervisedAnomalyDataset object at 0x7f4488b646d0>, [{'training_split': 0.6, 'max_contamination_rate': 0.4, 'n_steps': 4}, 'semisupervised_training_split_multiple']], 'spambase': [<oab.data.semisupervised.SemisupervisedAnomalyDataset object at 0x7f4483b96f40>, [{'training_split': 0.6, 'max_contamination_rate': 0.4, 'n_steps': 4}, 'semisupervised_training_split_multiple']]}\n"
     ]
    }
   ],
   "source": [
    "#ID 17(4)\n",
    "benchmarking_datasets={}\n",
    "\n",
    "for (x,y) in datasets_ad.items():\n",
    "    benchmarking_datasets[x]=[y,sampling_params_current_run]\n",
    "\n",
    "\n",
    "print(benchmarking_datasets)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248c3ad",
   "metadata": {
    "id": "e248c3ad"
   },
   "source": [
    "The above dictionary <b>benchmarking_datasets</b> will be used for the Benchmarking as it contains all the information:\"\n",
    "    \n",
    "    \n",
    "    1.dataset_name\n",
    "    2.final_dataset_object(preprocessed and anomaly-converted)\n",
    "    3.sampling_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd1b0f",
   "metadata": {
    "id": "babd1b0f"
   },
   "source": [
    "\n",
    "\n",
    "# **5. ALGORITHM TRAINING AND TESTING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tjmqC3lBaMwX",
   "metadata": {
    "id": "tjmqC3lBaMwX"
   },
   "source": [
    "We first download and import algorithms used for anomaly decection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "OZFq2XBfaMRN",
   "metadata": {
    "id": "OZFq2XBfaMRN",
    "outputId": "120455e9-8fb6-470a-cc1e-6863866be335"
   },
   "outputs": [],
   "source": [
    "#ID 18(5)\n",
    "\n",
    "\n",
    "\n",
    "# import anomaly detection algorithms from pyod\n",
    "from pyod.models.ocsvm import OCSVM # fit and decision_function\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.vae import VAE\n",
    "### ADD your algo import here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637590bb",
   "metadata": {
    "id": "637590bb"
   },
   "source": [
    "Firstly, we define hyperparameters for all algorithms and choose for benchmarking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eef70b70",
   "metadata": {
    "id": "eef70b70"
   },
   "outputs": [],
   "source": [
    "#ID 19(5)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "### ADD YOUR OWN (HYPER)PARAMETERS AND THEIR VALUES FOR PRE-INSTALLED ALGOS###\n",
    "\n",
    "\n",
    " \n",
    "lst_benchmark_algorithms =[  \n",
    "    \n",
    "\n",
    "    {   \n",
    "       \"algo_module_name\": \"pyod.models.pca\" , \n",
    "       \"algo_class_name\": \"PCA\",\n",
    "       \"algo_name_in_result_table\": \"PCA\",\n",
    "       \"algo_parameters\":{'n_components': 0.9, 'svd_solver': 'full'},\n",
    "       \"fit\": {'method_name': 'fit', 'params': {}}, \n",
    "       \"decision_function\": {'method_name': 'decision_function', 'params': {}}\n",
    "    },\n",
    "    {\n",
    "      \"algo_module_name\": \"pyod.models.auto_encoder\"   ,\n",
    "       \"algo_class_name\": \"AutoEncoder\",\n",
    "       \"algo_name_in_result_table\": \"AutoEncoder\",\n",
    "       \"algo_parameters\":   {'verbose': 0, 'hidden_neurons': [6, 3, 3, 6], 'random_state': 42},\n",
    "        \"fit\": {'method_name': 'fit', 'params': {}}, \n",
    "       \"decision_function\": {'method_name': 'decision_function', 'params': {}}\n",
    "       }\n",
    "]\n",
    "\n",
    "\n",
    "'''       ### uncomment to use these algos below for benchmarking ###\n",
    "     {   \n",
    "       \"algo_module_name\": \"pyod.models.vae\" , \n",
    "       \"algo_class_name\": \"VAE\",\n",
    "       \"algo_name_in_result_table\": \"VAE\",\n",
    "       \"algo_parameters\":  {'encoder_neurons': [6, 3], 'decoder_neurons': [3, 6], 'verbose': 0, 'random_state': 42},\n",
    "       \"fit\": {'method_name': 'fit', 'params': {}}, \n",
    "       \"decision_function\": {'method_name': 'decision_function', 'params': {}}\n",
    "    }\n",
    "  ,\n",
    "{   \n",
    "       \"algo_module_name\": \"pyod.models.ocsvm\" , \n",
    "       \"algo_class_name\": \"OCSVM\",\n",
    "       \"algo_name_in_result_table\": \"OCSVM\",\n",
    "       \"algo_parameters\": {'degree': 3}, \n",
    "       \"fit\": {'method_name': 'fit', 'params': {}}, \n",
    "       \"decision_function\": {'method_name': 'decision_function', 'params': {}}\n",
    "    },\n",
    "   \n",
    "       {\n",
    "       \"algo_module_name\": \"pyod.models.iforest\",   \n",
    "       \"algo_class_name\": \"IForest\",\n",
    "       \"algo_name_in_result_table\": \"IForest\",   \n",
    "       \"algo_parameters\": {'random_state': 42} ,\n",
    "        \"fit\": {'method_name': 'fit', 'params': {}}, \n",
    "        \"decision_function\": {'method_name':'decision_function', 'params': {}}\n",
    "        }\n",
    "\n",
    "'''   \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "### ADD OWN ALGORITHM(S)  with algorithm specifications as shown above for OAB algorithms ###   \n",
    "\n",
    "own_algorithms=[]   #add to this list e.g. { \"algo_module_name\": \"own_algo\" , \"algo_class_name\": \"ownAlgoClass\",..........\"decision_function\": {'method_name': 'decision_function', 'params': {}}} \n",
    "\n",
    "lst_benchmark_algorithms.extend(own_algorithms)\n",
    "\n",
    "\n",
    "#seeds defined for ths benchmark run for obtaining consistent results \n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de5187",
   "metadata": {
    "id": "00de5187"
   },
   "source": [
    "<b>LOAD YOUR RECIPE</b> to be repdroduced and use it in the current benchmark run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38327e77",
   "metadata": {
    "id": "38327e77",
    "outputId": "7a093a77-07f8-4945-ee16-c701c9fd3a56",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annthyroid:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "      - 0\r\n",
      "      - 1\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels: 0\r\n",
      "      anomaly_labels:\r\n",
      "- sampling:\r\n",
      "    semisupervised_training_split_multiple:\r\n",
      "      training_split: 0.7\r\n",
      "      max_contamination_rate: 0.5\r\n",
      "      n_steps: 1\r\n",
      "      \r\n",
      "myTabularDataset2:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "      - 0\r\n",
      "      - 1\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels: 0\r\n",
      "      anomaly_labels:\r\n",
      "- sampling:\r\n",
      "    semisupervised_training_split_multiple:\r\n",
      "      training_split: 0.7\r\n",
      "      max_contamination_rate: 0.5\r\n",
      "      n_steps: 1\r\n",
      "pyod.models.vae:\r\n",
      "- algo_name\r\n",
      "- init:\r\n",
      "    params:\r\n",
      "      encoder_neurons:\r\n",
      "      - 6\r\n",
      "      - 3\r\n",
      "      decoder_neurons:\r\n",
      "      - 3\r\n",
      "      - 6\r\n",
      "      verbose: 0\r\n",
      "      random_state: 42\r\n",
      "  fit:\r\n",
      "    method_name: fit\r\n",
      "    params: {}\r\n",
      "  decision_function:\r\n",
      "    method_name: decision_function\r\n",
      "    params: {}\r\n",
      "- VAE\r\n",
      "seed:\r\n",
      "  - 42\r\n"
     ]
    }
   ],
   "source": [
    "#ID 20(5)           # Execute this cell only when you already have a recipe file  to load from      \n",
    "\n",
    "\n",
    "### ADD AN OPTIONAL RECIPE  PATH TO ADD TO THIS BENCHMARK RUN START ###   \n",
    "\n",
    "# Note: recipes of type \"semisupervised tabular(sst) \" i.e. of the format: \n",
    "#               \"timestamp-benchmark_name-sst-recipe.yaml\"\n",
    "# can only be used for benchmarking in this notebook.\n",
    "\n",
    "recipe_path=recipes_parent_path/\"Paper_B\"/\"20211206050133-Paper_B-sst-recipe.yaml\"\n",
    "\n",
    "### ADD AN OPTIONAL RECIPE  PATH TO ADD TO THIS BENCHMARK RUN END ###   \n",
    "    \n",
    "### UNCOMMENT ONLY IF NO NEW DATASETS WERE ADDED IN THE BENCHMARK EXCEPT FROM RECIPE START ###     \n",
    "\n",
    "#benchmarking_datasets={}\n",
    "#lst_benchmark_algorithms=[]\n",
    "\n",
    "### UNCOMMENT ONLY IF NO NEW DATASETS WERE ADDED IN THE BENCHMARK EXCEPT FROM RECIPE END ###  \n",
    "\n",
    "\n",
    "!cat {recipe_path} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e79bad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyod.models.vae----\n",
      "\n",
      "\n",
      "annthyroid------\n",
      "standard/custom preprocessing performed!\n",
      "transformed to anomaly dataset!\n",
      "\n",
      "myTabularDataset2------\n",
      "standard/custom preprocessing performed!\n",
      "transformed to anomaly dataset!\n"
     ]
    }
   ],
   "source": [
    "#ID 21(5)                # Execute this cell only when you already have a recipe file  to load from   \n",
    "    \n",
    "recipe_algos=data_from_recipe('algos',recipe_path) # all algo names from recipe extracted\n",
    "#print(f\"recipe_algos:\\n{recipe_algos}\")\n",
    "\n",
    "recipe_datasets=data_from_recipe('datasets',recipe_path) # all dataset info(anomaly dataset object/anomalydataset params/sampling params) is perfomed and obtained\n",
    "#print(f\"\\nrecipe_datasets:\\n{recipe_datasets}\")\n",
    " \n",
    "recipe_seed=data_from_recipe('seed',recipe_path)  # obtained seeds to feed in this benchmark \n",
    "seed=recipe_seed   # seed of current benchmark is overwritten by recipe seed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ef08908",
   "metadata": {
    "id": "4ef08908",
    "outputId": "55a09390-24c1-47e1-bab9-37a84891604e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets obtained from recipe:\n",
      "annthyroid\n",
      "myTabularDataset2\n",
      "\n",
      "Algos obtained from recipe:\n",
      "pyod.models.vae\n",
      "\n",
      "All Datasets for this benchmark run:\n",
      "myTabularDataset\n",
      "spambase\n",
      "annthyroid\n",
      "myTabularDataset2\n",
      "\n",
      "All algos for this benchmark run:\n",
      "pyod.models.pca\n",
      "pyod.models.auto_encoder\n",
      "pyod.models.vae\n"
     ]
    }
   ],
   "source": [
    "#ID 22(5)    \n",
    "\n",
    "\n",
    "\n",
    "# adding recipe_datasets to benchmarking_datasets\n",
    "for dataset_name in recipe_datasets:\n",
    "    benchmarking_datasets[dataset_name]=recipe_datasets[dataset_name][:2]\n",
    "#print(f\"benchmarking_datasets: {benchmarking_datasets}\") \n",
    "                                       \n",
    "#adding algos from recipe_algos to lst_benchmarking_algos\n",
    "for algo in recipe_algos:\n",
    "    lst_benchmark_algorithms.append(algo)\n",
    "\n",
    "#print(lst_benchmark_algorithms)\n",
    "\n",
    "print(\"\\nDatasets obtained from recipe:\")    \n",
    "for dataset_name in recipe_datasets:\n",
    "    print(dataset_name)\n",
    "\n",
    "\n",
    "print(\"\\nAlgos obtained from recipe:\") \n",
    "for algo_name in recipe_algos:\n",
    "    print(algo_name['algo_module_name'])\n",
    "    \n",
    "  \n",
    "print(\"\\nAll Datasets for this benchmark run:\")    \n",
    "for dataset_name in benchmarking_datasets:\n",
    "    print(dataset_name)\n",
    "\n",
    "    \n",
    " \n",
    "print(\"\\nAll algos for this benchmark run:\")\n",
    "for algo in lst_benchmark_algorithms:\n",
    "    #print(algo)\n",
    "    print(algo['algo_module_name'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb76fd8",
   "metadata": {
    "id": "1eb76fd8"
   },
   "source": [
    "Now, For every benchmark dataset , we sample from that dataset to train the algorithms and then predict the outcomes for each dataset with each algortihm and then store results in a evaluation object, which is then added to the comparison object to show the final Benchmarking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0763aeb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0763aeb2",
    "outputId": "5b15015f-6bb6-4d74-f7ea-5ba4e7082c91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------myTabularDataset-------\n",
      "------PCA\n",
      "....\n",
      "\n",
      "Evaluation on dataset myTabularDataset with normal labels [0] and anomaly labels [1, 2].\n",
      "Total of 4 datasets. Per dataset:\n",
      "147 training instances, 163 test instances, training contamination rate 0.0, test contamination rate 0.3987730061349693.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.733 \t 0.032 \t\t roc_auc\n",
      "0.385 \t 0.104 \t\t adjusted_average_precision\n",
      "0.625 \t 0.065 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "------AutoEncoder\n",
      "....\n",
      "\n",
      "Evaluation on dataset myTabularDataset with normal labels [0] and anomaly labels [1, 2].\n",
      "Total of 4 datasets. Per dataset:\n",
      "147 training instances, 163 test instances, training contamination rate 0.0, test contamination rate 0.3987730061349693.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.806 \t 0.024 \t\t roc_auc\n",
      "0.516 \t 0.097 \t\t adjusted_average_precision\n",
      "0.704 \t 0.061 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "------VAE\n",
      "....\n",
      "\n",
      "Evaluation on dataset myTabularDataset with normal labels [0] and anomaly labels [1, 2].\n",
      "Total of 4 datasets. Per dataset:\n",
      "147 training instances, 163 test instances, training contamination rate 0.0, test contamination rate 0.3987730061349693.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.776 \t 0.027 \t\t roc_auc\n",
      "0.458 \t 0.107 \t\t adjusted_average_precision\n",
      "0.669 \t 0.067 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "-------spambase-------\n",
      "------PCA\n",
      "....\n",
      "\n",
      "Evaluation on dataset spambase with normal labels [0] and anomaly labels [1].\n",
      "Total of 4 datasets. Per dataset:\n",
      "1516 training instances, 1686 test instances, training contamination rate 0.0, test contamination rate 0.3997627520759193.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.812 \t 0.013 \t\t roc_auc\n",
      "0.571 \t 0.016 \t\t adjusted_average_precision\n",
      "0.742 \t 0.010 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "------AutoEncoder\n",
      "....\n",
      "\n",
      "Evaluation on dataset spambase with normal labels [0] and anomaly labels [1].\n",
      "Total of 4 datasets. Per dataset:\n",
      "1516 training instances, 1686 test instances, training contamination rate 0.0, test contamination rate 0.3997627520759193.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.811 \t 0.013 \t\t roc_auc\n",
      "0.570 \t 0.016 \t\t adjusted_average_precision\n",
      "0.741 \t 0.010 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "------VAE\n",
      "....\n",
      "\n",
      "Evaluation on dataset spambase with normal labels [0] and anomaly labels [1].\n",
      "Total of 4 datasets. Per dataset:\n",
      "1516 training instances, 1686 test instances, training contamination rate 0.0, test contamination rate 0.3997627520759193.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.812 \t 0.013 \t\t roc_auc\n",
      "0.571 \t 0.016 \t\t adjusted_average_precision\n",
      "0.742 \t 0.010 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "-------annthyroid-------\n",
      "------PCA\n",
      ".\n",
      "\n",
      "Evaluation on dataset annthyroid with normal labels [0] and anomaly labels [1.0].\n",
      "Total of 1 datasets. Per dataset:\n",
      "4569 training instances, 2493 test instances, training contamination rate 0.0, test contamination rate 0.21419975932611313.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.783 \t 0.000 \t\t roc_auc\n",
      "0.468 \t 0.000 \t\t adjusted_average_precision\n",
      "0.581 \t 0.000 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "------AutoEncoder\n",
      ".\n",
      "\n",
      "Evaluation on dataset annthyroid with normal labels [0] and anomaly labels [1.0].\n",
      "Total of 1 datasets. Per dataset:\n",
      "4569 training instances, 2493 test instances, training contamination rate 0.0, test contamination rate 0.21419975932611313.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.809 \t 0.000 \t\t roc_auc\n",
      "0.493 \t 0.000 \t\t adjusted_average_precision\n",
      "0.601 \t 0.000 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "------VAE\n",
      ".\n",
      "\n",
      "Evaluation on dataset annthyroid with normal labels [0] and anomaly labels [1.0].\n",
      "Total of 1 datasets. Per dataset:\n",
      "4569 training instances, 2493 test instances, training contamination rate 0.0, test contamination rate 0.21419975932611313.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.810 \t 0.000 \t\t roc_auc\n",
      "0.493 \t 0.000 \t\t adjusted_average_precision\n",
      "0.601 \t 0.000 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "-------myTabularDataset2-------\n",
      "------PCA\n",
      ".\n",
      "\n",
      "Evaluation on dataset myTabularDataset2 with normal labels [0] and anomaly labels [1, 2].\n",
      "Total of 1 datasets. Per dataset:\n",
      "171 training instances, 148 test instances, training contamination rate 0.0, test contamination rate 0.5.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.713 \t 0.000 \t\t roc_auc\n",
      "0.406 \t 0.000 \t\t adjusted_average_precision\n",
      "0.699 \t 0.000 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "------AutoEncoder\n",
      ".\n",
      "\n",
      "Evaluation on dataset myTabularDataset2 with normal labels [0] and anomaly labels [1, 2].\n",
      "Total of 1 datasets. Per dataset:\n",
      "171 training instances, 148 test instances, training contamination rate 0.0, test contamination rate 0.5.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.813 \t 0.000 \t\t roc_auc\n",
      "0.568 \t 0.000 \t\t adjusted_average_precision\n",
      "0.781 \t 0.000 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "------VAE\n",
      ".\n",
      "\n",
      "Evaluation on dataset myTabularDataset2 with normal labels [0] and anomaly labels [1, 2].\n",
      "Total of 1 datasets. Per dataset:\n",
      "171 training instances, 148 test instances, training contamination rate 0.0, test contamination rate 0.5.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.772 \t 0.000 \t\t roc_auc\n",
      "0.488 \t 0.000 \t\t adjusted_average_precision\n",
      "0.741 \t 0.000 \t\t precision_recall_auc\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ID 23(5)\n",
    "co = ComparisonObject()\n",
    "\n",
    "for dataset_name in list(benchmarking_datasets.keys()):\n",
    "    print(f'-------{dataset_name}-------') \n",
    "   \n",
    "    #print(mvtec_ad_own_datasets_list)\n",
    "    for alg in lst_benchmark_algorithms:\n",
    "        \n",
    "        print(\"------\"+alg[\"algo_class_name\"])\n",
    "        eval_obj = EvaluationObject(algorithm_name=alg[\"algo_name_in_result_table\"])\n",
    "        \n",
    "        \n",
    "        sampling_type=benchmarking_datasets[dataset_name][1][1]\n",
    "        sampling_params=benchmarking_datasets[dataset_name][1][0]\n",
    "        #print(sampling_type,sampling_params)\n",
    "        for (x_train, x_test, y_test),sample_config in sample_semisupervised(dataset_name,sampling_type,sampling_params,benchmarking_datasets[dataset_name][0]):\n",
    "                 \n",
    "                torch.manual_seed(seed)\n",
    "                random.seed(seed)\n",
    "                tf.random.set_seed(seed)\n",
    "                np.random.seed(seed) \n",
    "                os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "                torch.cuda.manual_seed(seed)\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "                torch.backends.cudnn.deterministic = True\n",
    "                torch.backends.cudnn.benchmark = False\n",
    "                torch.use_deterministic_algorithms(True)\n",
    "                \n",
    "                \n",
    "                mod = __import__(alg[\"algo_module_name\"], fromlist=[alg[\"algo_class_name\"]])\n",
    "                algo = getattr(mod, alg[\"algo_class_name\"])(**alg['algo_parameters'])        \n",
    "                print('.', end='') # update to see progress \n",
    "                getattr(algo,alg[\"fit\"][\"method_name\"])(x_train, **alg[\"fit\"][\"params\"])  # fitting algo\n",
    "                pred = getattr(algo,alg[\"decision_function\"][\"method_name\"])(x_test, **alg[\"decision_function\"][\"params\"]) # decision functions\n",
    "                eval_obj.add(ground_truth=y_test, prediction=pred, description=sample_config)  \n",
    "        print(\"\\n\")    \n",
    "        eval_desc = eval_obj.evaluate(print=True, metrics=['roc_auc', 'adjusted_average_precision', 'precision_recall_auc'])\n",
    "        co.add_evaluation(eval_desc)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c585e",
   "metadata": {
    "id": "501c585e"
   },
   "source": [
    "# **6. EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57beebf",
   "metadata": {
    "id": "e57beebf"
   },
   "source": [
    "Here , we will see how different metrics can be selected when evaluating an algorithm's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05bc232",
   "metadata": {
    "id": "d05bc232"
   },
   "source": [
    "In previous section while creating an evalutation description,  we used all metrics for evaluation:\n",
    "\n",
    "     eval_desc = eval_obj.evaluate(print=False, metrics=all_metrics)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f97869e2",
   "metadata": {
    "id": "f97869e2",
    "outputId": "5b7a7df0-c7d2-46cb-dbb7-06e29350ba0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roc_auc', 'average_precision', 'adjusted_average_precision', 'precision_n', 'adjusted_precision_n', 'precision_recall_auc']\n"
     ]
    }
   ],
   "source": [
    "#ID 24(6)\n",
    "\n",
    "# to use a subset, first see which ones are available\n",
    "\n",
    "print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c32d570c",
   "metadata": {
    "id": "c32d570c"
   },
   "outputs": [],
   "source": [
    "#ID 25(6)\n",
    "\n",
    "#### ADD YOUR OWN NUMBER OF METRICS ###\n",
    "\n",
    "#Then we can  select an arbitrary subset\n",
    "metrics=['roc_auc', 'precision_recall_auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39507d64",
   "metadata": {
    "id": "39507d64"
   },
   "source": [
    "# **7. SHOW BENCHMARK RESULTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97932df",
   "metadata": {
    "id": "c97932df"
   },
   "source": [
    "We compare by printing, the results of the evaluations of different Algo-Dataset combinations.\n",
    "\n",
    "\\[Latex version: bold for highest, italics for second highest, ?\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e3a61a8",
   "metadata": {
    "id": "6e3a61a8",
    "outputId": "f5f765eb-3f2a-463b-b266-e00924cad805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "             myTabularDataset  spambase  annthyroid  myTabularDataset2  \\\n",
      "PCA                  0.732928  0.811608    0.783386           0.713112   \n",
      "AutoEncoder          0.805808  0.811124    0.809295           0.813367   \n",
      "VAE                  0.776060  0.811939    0.810223           0.772462   \n",
      "Average              0.771599  0.811557    0.800968           0.766314   \n",
      "\n",
      "              Average  \n",
      "PCA          0.760258  \n",
      "AutoEncoder  0.809899  \n",
      "VAE          0.792671  \n",
      "Average           NaN  \n",
      "For adjusted_average_precision:\n",
      "             myTabularDataset  spambase  annthyroid  myTabularDataset2  \\\n",
      "PCA                  0.385346  0.570856    0.467751           0.405694   \n",
      "AutoEncoder          0.515698  0.569862    0.492707           0.567927   \n",
      "VAE                  0.457699  0.571325    0.492887           0.488129   \n",
      "Average              0.452914  0.570681    0.484448           0.487250   \n",
      "\n",
      "              Average  \n",
      "PCA          0.457412  \n",
      "AutoEncoder  0.536549  \n",
      "VAE          0.502510  \n",
      "Average           NaN  \n",
      "For precision_recall_auc:\n",
      "             myTabularDataset  spambase  annthyroid  myTabularDataset2  \\\n",
      "PCA                  0.625230  0.742036    0.581379           0.699435   \n",
      "AutoEncoder          0.704050  0.741439    0.600989           0.781006   \n",
      "VAE                  0.668819  0.742318    0.601130           0.740786   \n",
      "Average              0.666033  0.741931    0.594500           0.740409   \n",
      "\n",
      "              Average  \n",
      "PCA          0.662020  \n",
      "AutoEncoder  0.706871  \n",
      "VAE          0.688263  \n",
      "Average           NaN  \n"
     ]
    }
   ],
   "source": [
    "#ID 26(7)\n",
    "\n",
    "# print results in easily readable format\n",
    "co.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f3b0f46",
   "metadata": {
    "id": "6f3b0f46",
    "outputId": "3d4a09b9-cfd0-4ab8-c2a2-064072fc2f4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "            myTabularDataset      spambase    annthyroid myTabularDataset2  \\\n",
      "PCA             0.733+-0.032  0.812+-0.013  0.783+-0.000      0.713+-0.000   \n",
      "AutoEncoder     0.806+-0.024  0.811+-0.013  0.809+-0.000      0.813+-0.000   \n",
      "VAE             0.776+-0.027  0.812+-0.013  0.810+-0.000      0.772+-0.000   \n",
      "Average                0.772         0.812         0.801             0.766   \n",
      "\n",
      "              Average  \n",
      "PCA          0.760258  \n",
      "AutoEncoder  0.809899  \n",
      "VAE          0.792671  \n",
      "Average           NaN  \n",
      "\n",
      "For adjusted_average_precision:\n",
      "            myTabularDataset      spambase    annthyroid myTabularDataset2  \\\n",
      "PCA             0.385+-0.104  0.571+-0.016  0.468+-0.000      0.406+-0.000   \n",
      "AutoEncoder     0.516+-0.097  0.570+-0.016  0.493+-0.000      0.568+-0.000   \n",
      "VAE             0.458+-0.107  0.571+-0.016  0.493+-0.000      0.488+-0.000   \n",
      "Average                0.453         0.571         0.484             0.487   \n",
      "\n",
      "              Average  \n",
      "PCA          0.457412  \n",
      "AutoEncoder  0.536549  \n",
      "VAE          0.502510  \n",
      "Average           NaN  \n",
      "\n",
      "For precision_recall_auc:\n",
      "            myTabularDataset      spambase    annthyroid myTabularDataset2  \\\n",
      "PCA             0.625+-0.065  0.742+-0.010  0.581+-0.000      0.699+-0.000   \n",
      "AutoEncoder     0.704+-0.061  0.741+-0.010  0.601+-0.000      0.781+-0.000   \n",
      "VAE             0.669+-0.067  0.742+-0.010  0.601+-0.000      0.741+-0.000   \n",
      "Average                0.666         0.742         0.594             0.740   \n",
      "\n",
      "              Average  \n",
      "PCA          0.662020  \n",
      "AutoEncoder  0.706871  \n",
      "VAE          0.688263  \n",
      "Average           NaN  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ID 28(7)\n",
    "\n",
    "# print results in easily readable format with standard deviations\n",
    "co.print_results(include_stdevs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sZfxpO-5UPfp",
   "metadata": {
    "id": "sZfxpO-5UPfp",
    "outputId": "4452e949-87c2-4a00-f543-96111c430ae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "\\begin{center}\n",
      "\\begin{tabular}{  c c c c c c  }\n",
      "  & myTabularDataset & spambase & annthyroid & myTabularDataset2 & Average \\\\\n",
      "  PCA & 0.733$\\pm$0.032 & \\textit{0.812$\\pm$0.013} & 0.783$\\pm$0.000 & 0.713$\\pm$0.000 & 0.760 \\\\\n",
      "  AutoEncoder & \\textbf{0.806$\\pm$0.024} & 0.811$\\pm$0.013 & \\textit{0.809$\\pm$0.000} & \\textbf{0.813$\\pm$0.000} & \\textbf{0.810} \\\\\n",
      "  VAE & \\textit{0.776$\\pm$0.027} & \\textbf{0.812$\\pm$0.013} & \\textbf{0.810$\\pm$0.000} & \\textit{0.772$\\pm$0.000} & \\textit{0.793} \\\\\n",
      "  Average & 0.772 & 0.812 & 0.801 & 0.766 &    \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "For adjusted_average_precision:\n",
      "\\begin{center}\n",
      "\\begin{tabular}{  c c c c c c  }\n",
      "  & myTabularDataset & spambase & annthyroid & myTabularDataset2 & Average \\\\\n",
      "  PCA & 0.385$\\pm$0.104 & \\textit{0.571$\\pm$0.016} & 0.468$\\pm$0.000 & 0.406$\\pm$0.000 & 0.457 \\\\\n",
      "  AutoEncoder & \\textbf{0.516$\\pm$0.097} & 0.570$\\pm$0.016 & \\textit{0.493$\\pm$0.000} & \\textbf{0.568$\\pm$0.000} & \\textbf{0.537} \\\\\n",
      "  VAE & \\textit{0.458$\\pm$0.107} & \\textbf{0.571$\\pm$0.016} & \\textbf{0.493$\\pm$0.000} & \\textit{0.488$\\pm$0.000} & \\textit{0.503} \\\\\n",
      "  Average & 0.453 & 0.571 & 0.484 & 0.487 &    \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "For precision_recall_auc:\n",
      "\\begin{center}\n",
      "\\begin{tabular}{  c c c c c c  }\n",
      "  & myTabularDataset & spambase & annthyroid & myTabularDataset2 & Average \\\\\n",
      "  PCA & 0.625$\\pm$0.065 & \\textit{0.742$\\pm$0.010} & 0.581$\\pm$0.000 & 0.699$\\pm$0.000 & 0.662 \\\\\n",
      "  AutoEncoder & \\textbf{0.704$\\pm$0.061} & 0.741$\\pm$0.010 & \\textit{0.601$\\pm$0.000} & \\textbf{0.781$\\pm$0.000} & \\textbf{0.707} \\\\\n",
      "  VAE & \\textit{0.669$\\pm$0.067} & \\textbf{0.742$\\pm$0.010} & \\textbf{0.601$\\pm$0.000} & \\textit{0.741$\\pm$0.000} & \\textit{0.688} \\\\\n",
      "  Average & 0.666 & 0.742 & 0.594 & 0.740 &    \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ID 29(7)\n",
    "\n",
    "co.print_latex(include_stdevs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ZlXKXx64aAl",
   "metadata": {
    "id": "1ZlXKXx64aAl"
   },
   "source": [
    "# **8. REPRODUCIBILITY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eBbV_ZNYiiC",
   "metadata": {
    "id": "7eBbV_ZNYiiC"
   },
   "source": [
    " ## **8.1 Creating recipes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zWTe3BI5Gfyz",
   "metadata": {
    "id": "zWTe3BI5Gfyz"
   },
   "source": [
    "This section shows **how `oab` can be used to make sampling results easily reproducible** .\n",
    " \n",
    "\n",
    "`yaml` files play an integral role in making reproducibility work, as they store the operations and parameters performed on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c0a73",
   "metadata": {
    "id": "dc4c0a73"
   },
   "source": [
    "We will see how to produce a recipe(.yaml) of the Benchmarkrun already performed  in <b>#ID 23(5)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827f5fb",
   "metadata": {
    "id": "e827f5fb"
   },
   "source": [
    "In <b>#ID 11(3) #ID 13(3) #ID 15(4)</b>,  We already performed operations on own datasets and OAB's datasets, and then already stored the daasets information as we can see below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff4c5cf3",
   "metadata": {
    "id": "ff4c5cf3",
    "outputId": "eae501b9-9947-4f1a-c911-d3000fa33f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myTabularDataset:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels:\r\n",
      "      - 0\r\n",
      "      anomaly_labels:\r\n",
      "- sampling:\r\n",
      "    semisupervised_training_split_multiple:\r\n",
      "      training_split: 0.6\r\n",
      "      max_contamination_rate: 0.4\r\n",
      "      n_steps: 4\r\n",
      "spambase:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels:\r\n",
      "      - 0\r\n",
      "      anomaly_labels:\r\n",
      "- sampling:\r\n",
      "    semisupervised_training_split_multiple:\r\n",
      "      training_split: 0.6\r\n",
      "      max_contamination_rate: 0.4\r\n",
      "      n_steps: 4\r\n"
     ]
    }
   ],
   "source": [
    "#ID 30(8)\n",
    "!cat {new_recipe_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca638c8",
   "metadata": {
    "id": "5ca638c8"
   },
   "source": [
    "Now, we will store the information of  datasets and algorithms information from <b>Paper_B's</b> recipe\n",
    "and only of the algorithms of this benchmark in the new recipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48159739",
   "metadata": {
    "id": "48159739"
   },
   "outputs": [],
   "source": [
    "#ID 31(8)\n",
    "\n",
    "\n",
    "# adding datasets from recipe used in in benchmark run in #ID 23(5)\n",
    "for dataset_name in recipe_datasets:\n",
    "    \n",
    "    \n",
    "    #storing anomaly parameters\n",
    "    dataset_info_store(dataset_name,new_recipe_path,info_type='anomaly_dataset',content=recipe_datasets[dataset_name][0].normal_labels)\n",
    "    \n",
    "    # storing preprocesing parameters\n",
    "    dataset_info_store(dataset_name,new_recipe_path,info_type='standard_functions',content=recipe_datasets[dataset_name][3]) \n",
    "    #dataset_info_store(dataset_name,new_recipe_path,info_type='custom_functions',content=recipe_datasets[dataset_name][4]) \n",
    "    \n",
    "    \n",
    "    #storing sampling parameters\n",
    "    sampling_data=recipe_datasets[dataset_name][1]\n",
    "    dataset_info_store(dataset_name,new_recipe_path,'sampling',content=sampling_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe5ffa",
   "metadata": {
    "id": "5dbe5ffa"
   },
   "source": [
    "Now,we will store information about <b>Algorithms and their hyperparameters</b> in the recipe(.yaml) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce1e6fd1",
   "metadata": {
    "id": "ce1e6fd1"
   },
   "outputs": [],
   "source": [
    "# 32(8)\n",
    "for algo in lst_benchmark_algorithms:\n",
    "    \n",
    "    x=algo[\"algo_module_name\"]\n",
    "    y=['algo_name',\n",
    "         \n",
    "         {\n",
    "         'init': \n",
    "          \n",
    "               {\n",
    "\n",
    "       'params':algo[\"algo_parameters\"]\n",
    "          \n",
    "               },\n",
    "        \n",
    "        'fit':algo[\"fit\"]   \n",
    "        ,\n",
    "\n",
    "        'decision_function':algo[\"decision_function\"]\n",
    "         },\n",
    "         \n",
    "         algo[\"algo_class_name\"]\n",
    "        \n",
    "        ]\n",
    "                 \n",
    "     \n",
    "    yaml=YAML(typ='rt')\n",
    "    yaml_content = yaml.load(Path(\"./\") / new_recipe_path)\n",
    "    yaml_content[x]=y\n",
    "    yaml_content['seed']=[seed]          # adding seed to new recipe\n",
    "    yaml.dump(yaml_content, Path(\"./\") /new_recipe_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ef1QSaxHGgr0",
   "metadata": {
    "id": "Ef1QSaxHGgr0"
   },
   "source": [
    "In **f\"{time}-{benchmark_name}-{benchmark_type}-recipe.yaml\"**, we now see the sampling parameters, anomaly- dataset-conversion parameters, hyperparamters along with the algorithms for \"unsupervised_multiple_with_training_split\". If sampling is done in a different scenario, e.g., unsupervised multiple, this would also be stored in f\"{benchmark_name}/{time}_{benchmark_name}_recipe.yaml\" using a different key in the sampling dict.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a989a8f5",
   "metadata": {
    "id": "a989a8f5",
    "outputId": "212bd717-5871-467b-b30c-a6a6964ad482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myTabularDataset:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels:\r\n",
      "      - 0\r\n",
      "      anomaly_labels:\r\n",
      "- sampling:\r\n",
      "    semisupervised_training_split_multiple:\r\n",
      "      training_split: 0.6\r\n",
      "      max_contamination_rate: 0.4\r\n",
      "      n_steps: 4\r\n",
      "spambase:\r\n",
      "- dataset\r\n",
      "- standard_functions:\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels:\r\n",
      "      - 0\r\n",
      "      anomaly_labels:\r\n",
      "- sampling:\r\n",
      "    semisupervised_training_split_multiple:\r\n",
      "      training_split: 0.6\r\n",
      "      max_contamination_rate: 0.4\r\n",
      "      n_steps: 4\r\n",
      "annthyroid:\r\n",
      "- dataset\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels:\r\n",
      "      - 0\r\n",
      "      anomaly_labels:\r\n",
      "- standard_functions:\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "      - 0\r\n",
      "      - 1\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- sampling:\r\n",
      "    training_split: 0.7\r\n",
      "    max_contamination_rate: 0.5\r\n",
      "    n_steps: 1\r\n",
      "myTabularDataset2:\r\n",
      "- dataset\r\n",
      "- anomaly_dataset:\r\n",
      "    arguments:\r\n",
      "      normal_labels:\r\n",
      "      - 0\r\n",
      "      anomaly_labels:\r\n",
      "- standard_functions:\r\n",
      "  - name: normalize_columns\r\n",
      "    parameters:\r\n",
      "      cols_to_normalize:\r\n",
      "      - 0\r\n",
      "      - 1\r\n",
      "  - name: treat_missing_values\r\n",
      "    parameters:\r\n",
      "      missing_value: np.nan\r\n",
      "      delete_attributes: true\r\n",
      "  - name: delete_duplicates\r\n",
      "    parameters: {}\r\n",
      "- sampling:\r\n",
      "    training_split: 0.7\r\n",
      "    max_contamination_rate: 0.5\r\n",
      "    n_steps: 1\r\n",
      "pyod.models.pca:\r\n",
      "- algo_name\r\n",
      "- init:\r\n",
      "    params:\r\n",
      "      n_components: 0.9\r\n",
      "      svd_solver: full\r\n",
      "  fit:\r\n",
      "    method_name: fit\r\n",
      "    params: {}\r\n",
      "  decision_function:\r\n",
      "    method_name: decision_function\r\n",
      "    params: {}\r\n",
      "- PCA\r\n",
      "seed:\r\n",
      "- 42\r\n",
      "pyod.models.auto_encoder:\r\n",
      "- algo_name\r\n",
      "- init:\r\n",
      "    params:\r\n",
      "      verbose: 0\r\n",
      "      hidden_neurons:\r\n",
      "      - 6\r\n",
      "      - 3\r\n",
      "      - 3\r\n",
      "      - 6\r\n",
      "      random_state: 42\r\n",
      "  fit:\r\n",
      "    method_name: fit\r\n",
      "    params: {}\r\n",
      "  decision_function:\r\n",
      "    method_name: decision_function\r\n",
      "    params: {}\r\n",
      "- AutoEncoder\r\n",
      "pyod.models.vae:\r\n",
      "- algo_name\r\n",
      "- init:\r\n",
      "    params:\r\n",
      "      encoder_neurons:\r\n",
      "      - 6\r\n",
      "      - 3\r\n",
      "      decoder_neurons:\r\n",
      "      - 3\r\n",
      "      - 6\r\n",
      "      verbose: 0\r\n",
      "      random_state: 42\r\n",
      "  fit:\r\n",
      "    method_name: fit\r\n",
      "    params: {}\r\n",
      "  decision_function:\r\n",
      "    method_name: decision_function\r\n",
      "    params: {}\r\n",
      "- VAE\r\n"
     ]
    }
   ],
   "source": [
    "#ID 33(8)\n",
    "!cat {new_recipe_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U7PSnotYJIr2",
   "metadata": {
    "id": "U7PSnotYJIr2"
   },
   "source": [
    "### 2. Reproducing the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71159158",
   "metadata": {
    "id": "71159158",
    "outputId": "d37a18ec-5285-46ad-b379-2147cb9d2c98",
    "scrolled": false
   },
   "source": [
    "To reproduce the recipe created in the previous section,\n",
    "we refer to <b>Section 5 #ID 20(5)</b> where we can reproduce the run as well as extend benchmarks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ieghRWumgNY9",
   "metadata": {
    "id": "ieghRWumgNY9"
   },
   "source": [
    "# **9. EXTEND EXISTING BENCHMARK(own algorithm)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1HfN4e3PMn0",
   "metadata": {
    "id": "i1HfN4e3PMn0"
   },
   "source": [
    "To extend the existing benchmark here basically means to add  our own algorithm to the benchmark and to show the comparison results of pre-installed algorithms while also loading our own dataset.\n",
    "\n",
    "\n",
    "1. We load the datasets. To know how to do that, we can refer to  **Section \"1. Data\" and \"2. Data Selection\"**\n",
    "2. Then, load own algorithm as we will see in the next sub-section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kAAV6vAOy9-k",
   "metadata": {
    "id": "kAAV6vAOy9-k"
   },
   "source": [
    "## **9.1 Loading own Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cT1MEn7zSta",
   "metadata": {
    "id": "3cT1MEn7zSta"
   },
   "source": [
    "In this subsection, you will see **how an own semisupervised anomaly detection algorithm** can easily be used within oab to be evaluated. We will see how a class representing an algorithm can be structured and how its performance is evaluated.\n",
    "\n",
    "Of course, this is not the only way to use the functionality provided by oab. We do consider it to be the simplest way however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "r7jxQhkcze5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7jxQhkcze5d",
    "outputId": "c7d5d85b-dee5-41cc-f24e-49da6f14594e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\r\n",
      "\r\n",
      "class RandomGuesserSemisupervised():\r\n",
      "\r\n",
      "    def fit(self, X_train):\r\n",
      "        pass\r\n",
      "      \r\n",
      "    def decision_function(self, X_test):\r\n",
      "        \"Assign a random number to each sample from the test set\"\r\n",
      "        n_samples = X_test.shape[0]\r\n",
      "        return np.random.randn(n_samples)\r\n"
     ]
    }
   ],
   "source": [
    "#ID 34(9)\n",
    "\n",
    "# download example algorithm and inspect content\n",
    "import wget\n",
    "wget.download('https://raw.githubusercontent.com/jandeller/test/main/RandomGuesserSemisupervised.py',f\"{recipes_parent_path}/RandomGuesserSemisupervised.py\")\n",
    "!cat {recipes_parent_path}/RandomGuesserSemisupervised.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enGFFsQpRJ2Y",
   "metadata": {
    "id": "enGFFsQpRJ2Y"
   },
   "source": [
    "The sample `RandomGuesser` algorithm shown here is - as the name suggests - a random guesser, i.e., it assigns random anomaly scores to the samples.\n",
    "\n",
    "An algorithm used for unsupervised anomaly detection needs to specify a `fit(x_train)` method for training and a `decision_function(x_test)` method for inference that returns an anomaly score per data point in the test set.\n",
    "\n",
    "It is of course possible to rename the method and field, use a method for accessing the anomaly scores, etc. Note that if this is done, the following code has to be changed accordingly. Adhering to the conventions described above (`fit(x_train)` and `decision_function(x_test)`) allows you to use the same interface as algorithms from [`PyOD`](https://pyod.readthedocs.io/en/latest/) as shown when [comparing algorithms using `oab`](https://colab.research.google.com/drive/1aV_itaYCJgzdZ1lQ7SUyHQ7z01xSPxDN?usp=sharing#scrollTo=QnAfCGTGL7xv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "Y4FYSMtIzsqZ",
   "metadata": {
    "id": "Y4FYSMtIzsqZ",
    "outputId": "ed57bff9-0489-4519-d07b-f19058da63a3"
   },
   "outputs": [],
   "source": [
    "#ID 35(9)\n",
    "# used imports from #ID 3(0),#ID 18(5)\n",
    "#used sampling parameters from #ID 15(4)\n",
    "\n",
    "# and import the RandomGuesser\n",
    "from notebooks.benchmark_tabular.RandomGuesserSemisupervised import RandomGuesserSemisupervised\n",
    " \n",
    "own_algorithms=[{\n",
    "    \n",
    "       ### ADD YOUR OWN ALGO DETAILS IN THIS FORM ###\n",
    "       \"algo_module_name\": \"RandomGuesserSemisupervised\",   \n",
    "       \"algo_class_name\": \"RandomGuesserSemisupervised\",\n",
    "       \"algo_name_in_result_table\": \"RandomGuesserSemisupervised\",\n",
    "       \"algo_parameters\": {},\n",
    "        \"fit\": {'method_name': 'fit', 'params': {}}, \n",
    "        \"decision_function\": {'method_name': 'decision_function', 'params': {}}\n",
    "        }]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e1161",
   "metadata": {},
   "source": [
    "The `own_algorithms` list in the above cell #ID 35(9) can be added to lst_benchmarking_algos as mentioned in #ID 19(5) to use this algorithm in a benchmark run shown in #ID 23(5) along with other algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "RU5K81TC0PmQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RU5K81TC0PmQ",
    "outputId": "e0c6a548-76be-46f2-f4be-4696efa51fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . \n",
      "\n",
      "Evaluation on dataset myTabularDataset with normal labels [0] and anomaly labels [1, 2].\n",
      "Total of 4 datasets. Per dataset:\n",
      "147 training instances, 163 test instances, training contamination rate 0.0, test contamination rate 0.3987730061349693.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.516 \t 0.023 \t\t roc_auc\n",
      "0.047 \t 0.054 \t\t adjusted_average_precision\n",
      "0.417 \t 0.034 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      ". . . . \n",
      "\n",
      "Evaluation on dataset spambase with normal labels [0] and anomaly labels [1].\n",
      "Total of 4 datasets. Per dataset:\n",
      "1516 training instances, 1686 test instances, training contamination rate 0.0, test contamination rate 0.3997627520759193.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.505 \t 0.009 \t\t roc_auc\n",
      "0.005 \t 0.021 \t\t adjusted_average_precision\n",
      "0.402 \t 0.012 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      ". . . . \n",
      "\n",
      "Evaluation on dataset annthyroid with normal labels [0] and anomaly labels [1.0].\n",
      "Total of 4 datasets. Per dataset:\n",
      "3916 training instances, 3146 test instances, training contamination rate 0.0, test contamination rate 0.16973935155753336.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.496 \t 0.012 \t\t roc_auc\n",
      "0.002 \t 0.011 \t\t adjusted_average_precision\n",
      "0.171 \t 0.009 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      ". . . . \n",
      "\n",
      "Evaluation on dataset myTabularDataset2 with normal labels [0] and anomaly labels [1, 2].\n",
      "Total of 4 datasets. Per dataset:\n",
      "147 training instances, 163 test instances, training contamination rate 0.0, test contamination rate 0.3987730061349693.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.516 \t 0.023 \t\t roc_auc\n",
      "0.047 \t 0.054 \t\t adjusted_average_precision\n",
      "0.417 \t 0.034 \t\t precision_recall_auc\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ID 36(9)\n",
    "\n",
    "\n",
    "#  A comparison object is created for comparing the evaluations of different Algo-Dataset combinations\n",
    "co = ComparisonObject()\n",
    "\n",
    "for dataset_name in benchmarking_datasets:\n",
    "  # evaluate the random guesser\n",
    "  eval_obj = EvaluationObject(algorithm_name=\"RandomGuesser\")\n",
    "  for (X_train, X_test, y_test), settings in benchmarking_datasets[dataset_name][0].sample_multiple_with_training_split(training_split=training_split, \n",
    "                                                                  max_contamination_rate=max_contamination_rate, \n",
    "                                                                  n_steps=n_steps):\n",
    "      print(\".\", end=\" \") # update to see progress\n",
    "      rg = RandomGuesserSemisupervised()\n",
    "      rg.fit(X_train) # data is fitted to RandomGuesser\n",
    "      pred = rg.decision_function(X_test) # and decision_scores_ is accessed\n",
    "      eval_obj.add(y_test, pred, settings)\n",
    "  print(\"\\n\")\n",
    "  eval_desc = eval_obj.evaluate(metrics=['roc_auc', 'adjusted_average_precision', 'precision_recall_auc'])\n",
    "  # added to comparison object\n",
    "  co.add_evaluation(eval_desc)\n",
    "  print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CHYWyeKKxs9M",
   "metadata": {
    "id": "CHYWyeKKxs9M"
   },
   "source": [
    "As in the above code, We store the evaluations of our own algorithm in evaluation object which is then added to comparison object.Similarly, we can create evaluation objects for other algorithms and add them to comparison object for final benchmarking  as shown in Section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k-8sW1dvTcNj",
   "metadata": {
    "id": "k-8sW1dvTcNj"
   },
   "source": [
    "Finally, we show below the benchmarking results of our algorithm as described in \"**Section 7. Show Benchmarking Results**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "_RxojZg5zDhb",
   "metadata": {
    "id": "_RxojZg5zDhb",
    "outputId": "367c88df-594e-4e66-c709-6dc7003c0ac5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "               myTabularDataset  spambase  annthyroid  myTabularDataset2  \\\n",
      "PCA                    0.732928  0.811608    0.783386           0.713112   \n",
      "AutoEncoder            0.805808  0.811124    0.809295           0.813367   \n",
      "VAE                    0.776060  0.811939    0.810223           0.772462   \n",
      "RandomGuesser          0.516209  0.505419    0.496305           0.516209   \n",
      "Average                0.707751  0.735023    0.724802           0.703787   \n",
      "\n",
      "                Average  \n",
      "PCA            0.760258  \n",
      "AutoEncoder    0.809899  \n",
      "VAE            0.792671  \n",
      "RandomGuesser  0.508535  \n",
      "Average             NaN  \n",
      "For adjusted_average_precision:\n",
      "               myTabularDataset  spambase  annthyroid  myTabularDataset2  \\\n",
      "PCA                    0.385346  0.570856    0.467751           0.405694   \n",
      "AutoEncoder            0.515698  0.569862    0.492707           0.567927   \n",
      "VAE                    0.457699  0.571325    0.492887           0.488129   \n",
      "RandomGuesser          0.046783  0.005494    0.002468           0.046783   \n",
      "Average                0.351381  0.429384    0.363953           0.377133   \n",
      "\n",
      "                Average  \n",
      "PCA            0.457412  \n",
      "AutoEncoder    0.536549  \n",
      "VAE            0.502510  \n",
      "RandomGuesser  0.025382  \n",
      "Average             NaN  \n",
      "For precision_recall_auc:\n",
      "               myTabularDataset  spambase  annthyroid  myTabularDataset2  \\\n",
      "PCA                    0.625230  0.742036    0.581379           0.699435   \n",
      "AutoEncoder            0.704050  0.741439    0.600989           0.781006   \n",
      "VAE                    0.668819  0.742318    0.601130           0.740786   \n",
      "RandomGuesser          0.417144  0.401851    0.170813           0.417144   \n",
      "Average                0.603811  0.656911    0.488578           0.659593   \n",
      "\n",
      "                Average  \n",
      "PCA            0.662020  \n",
      "AutoEncoder    0.706871  \n",
      "VAE            0.688263  \n",
      "RandomGuesser  0.351738  \n",
      "Average             NaN  \n"
     ]
    }
   ],
   "source": [
    "#ID 37(9)\n",
    "\n",
    "# print results in easily readable format\n",
    "co.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1717c96b",
   "metadata": {
    "id": "1717c96b",
    "outputId": "98f83fc9-6661-4b94-c5df-c907831c3f94",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "              myTabularDataset      spambase    annthyroid myTabularDataset2  \\\n",
      "PCA               0.733+-0.032  0.812+-0.013  0.783+-0.000      0.713+-0.000   \n",
      "AutoEncoder       0.806+-0.024  0.811+-0.013  0.809+-0.000      0.813+-0.000   \n",
      "VAE               0.776+-0.027  0.812+-0.013  0.810+-0.000      0.772+-0.000   \n",
      "RandomGuesser     0.516+-0.023  0.505+-0.009  0.496+-0.012      0.516+-0.023   \n",
      "Average                  0.708         0.735         0.725             0.704   \n",
      "\n",
      "                Average  \n",
      "PCA            0.760258  \n",
      "AutoEncoder    0.809899  \n",
      "VAE            0.792671  \n",
      "RandomGuesser  0.508535  \n",
      "Average             NaN  \n",
      "\n",
      "For adjusted_average_precision:\n",
      "              myTabularDataset      spambase    annthyroid myTabularDataset2  \\\n",
      "PCA               0.385+-0.104  0.571+-0.016  0.468+-0.000      0.406+-0.000   \n",
      "AutoEncoder       0.516+-0.097  0.570+-0.016  0.493+-0.000      0.568+-0.000   \n",
      "VAE               0.458+-0.107  0.571+-0.016  0.493+-0.000      0.488+-0.000   \n",
      "RandomGuesser     0.047+-0.054  0.005+-0.021  0.002+-0.011      0.047+-0.054   \n",
      "Average                  0.351         0.429         0.364             0.377   \n",
      "\n",
      "                Average  \n",
      "PCA            0.457412  \n",
      "AutoEncoder    0.536549  \n",
      "VAE            0.502510  \n",
      "RandomGuesser  0.025382  \n",
      "Average             NaN  \n",
      "\n",
      "For precision_recall_auc:\n",
      "              myTabularDataset      spambase    annthyroid myTabularDataset2  \\\n",
      "PCA               0.625+-0.065  0.742+-0.010  0.581+-0.000      0.699+-0.000   \n",
      "AutoEncoder       0.704+-0.061  0.741+-0.010  0.601+-0.000      0.781+-0.000   \n",
      "VAE               0.669+-0.067  0.742+-0.010  0.601+-0.000      0.741+-0.000   \n",
      "RandomGuesser     0.417+-0.034  0.402+-0.012  0.171+-0.009      0.417+-0.034   \n",
      "Average                  0.604         0.657         0.489             0.660   \n",
      "\n",
      "                Average  \n",
      "PCA            0.662020  \n",
      "AutoEncoder    0.706871  \n",
      "VAE            0.688263  \n",
      "RandomGuesser  0.351738  \n",
      "Average             NaN  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ID 38(9)\n",
    "# print results in easily readable format with standard deviations\n",
    "co.print_results(include_stdevs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a75770e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "\\begin{center}\n",
      "\\begin{tabular}{  c c c c c c  }\n",
      "  & myTabularDataset & spambase & annthyroid & myTabularDataset2 & Average \\\\\n",
      "  PCA & 0.733$\\pm$0.032 & \\textit{0.812$\\pm$0.013} & 0.783$\\pm$0.000 & 0.713$\\pm$0.000 & 0.760 \\\\\n",
      "  AutoEncoder & \\textbf{0.806$\\pm$0.024} & 0.811$\\pm$0.013 & \\textit{0.809$\\pm$0.000} & \\textbf{0.813$\\pm$0.000} & \\textbf{0.810} \\\\\n",
      "  VAE & \\textit{0.776$\\pm$0.027} & \\textbf{0.812$\\pm$0.013} & \\textbf{0.810$\\pm$0.000} & \\textit{0.772$\\pm$0.000} & \\textit{0.793} \\\\\n",
      "  RandomGuesser & 0.516$\\pm$0.023 & 0.505$\\pm$0.009 & 0.496$\\pm$0.012 & 0.516$\\pm$0.023 & 0.509 \\\\\n",
      "  Average & 0.708 & 0.735 & 0.725 & 0.704 &    \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "For adjusted_average_precision:\n",
      "\\begin{center}\n",
      "\\begin{tabular}{  c c c c c c  }\n",
      "  & myTabularDataset & spambase & annthyroid & myTabularDataset2 & Average \\\\\n",
      "  PCA & 0.385$\\pm$0.104 & \\textit{0.571$\\pm$0.016} & 0.468$\\pm$0.000 & 0.406$\\pm$0.000 & 0.457 \\\\\n",
      "  AutoEncoder & \\textbf{0.516$\\pm$0.097} & 0.570$\\pm$0.016 & \\textit{0.493$\\pm$0.000} & \\textbf{0.568$\\pm$0.000} & \\textbf{0.537} \\\\\n",
      "  VAE & \\textit{0.458$\\pm$0.107} & \\textbf{0.571$\\pm$0.016} & \\textbf{0.493$\\pm$0.000} & \\textit{0.488$\\pm$0.000} & \\textit{0.503} \\\\\n",
      "  RandomGuesser & 0.047$\\pm$0.054 & 0.005$\\pm$0.021 & 0.002$\\pm$0.011 & 0.047$\\pm$0.054 & 0.025 \\\\\n",
      "  Average & 0.351 & 0.429 & 0.364 & 0.377 &    \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "For precision_recall_auc:\n",
      "\\begin{center}\n",
      "\\begin{tabular}{  c c c c c c  }\n",
      "  & myTabularDataset & spambase & annthyroid & myTabularDataset2 & Average \\\\\n",
      "  PCA & 0.625$\\pm$0.065 & \\textit{0.742$\\pm$0.010} & 0.581$\\pm$0.000 & 0.699$\\pm$0.000 & 0.662 \\\\\n",
      "  AutoEncoder & \\textbf{0.704$\\pm$0.061} & 0.741$\\pm$0.010 & \\textit{0.601$\\pm$0.000} & \\textbf{0.781$\\pm$0.000} & \\textbf{0.707} \\\\\n",
      "  VAE & \\textit{0.669$\\pm$0.067} & \\textbf{0.742$\\pm$0.010} & \\textbf{0.601$\\pm$0.000} & \\textit{0.741$\\pm$0.000} & \\textit{0.688} \\\\\n",
      "  RandomGuesser & 0.417$\\pm$0.034 & 0.402$\\pm$0.012 & 0.171$\\pm$0.009 & 0.417$\\pm$0.034 & 0.352 \\\\\n",
      "  Average & 0.604 & 0.657 & 0.489 & 0.660 &    \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ID 39(9)\n",
    "\n",
    "co.print_latex(include_stdevs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc535a8",
   "metadata": {
    "id": "0bc535a8"
   },
   "source": [
    "So,This was our example algorithm. Other algorithms can be used to run and extend benchmarks,  Please refer  to <b>#ID 19(5)</b>."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Unsupervised_Anomaly_Detection_on_Benchmark_Image_Data_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "oab",
   "language": "python",
   "name": "oab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
