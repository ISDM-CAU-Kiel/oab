{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100a6f8d",
   "metadata": {
    "id": "100a6f8d"
   },
   "source": [
    "\n",
    "## In this notebook, you will see all the steps sequentially performed to be able to utilize the complete functionality of OAB framework. The steps are as follows :\n",
    "0. SETUP\n",
    "1. DATA\n",
    "2. DATA SELECTION\n",
    "3. PREPROCESSING\n",
    "4. SAMPLING\n",
    "5. ALGORITHM TRAINING AND TESTING\n",
    "6. EVALUATION\n",
    "7. SHOW BENCHMARK RESULTS\n",
    "8. REPRODUCIBILTY\n",
    "9. EXTENDING THE BENCHMARK(with own Algorithm)\n",
    "\n",
    "This notebook focuses on <b>Semisupervised Image Data</b>. Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VXCXUMi0bRJA",
   "metadata": {
    "id": "VXCXUMi0bRJA"
   },
   "source": [
    "# **0. SETUP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kkhqUcFybZ56",
   "metadata": {
    "id": "kkhqUcFybZ56"
   },
   "source": [
    "`oab` framework can be integrated in your Python environment  as a PyPi package  using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9rc17bsZbbnb",
   "metadata": {
    "collapsed": true,
    "id": "9rc17bsZbbnb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 1(0)\n",
    "\n",
    "%%capture\n",
    "# pip install oab\n",
    "!pip install example-pkg-jd-kiel --extra-index-url=https://test.pypi.org/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb70c2",
   "metadata": {
    "id": "50eb70c2"
   },
   "source": [
    "## **0.1 NOTEBOOK AND CELL STRUCTURE** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4cbba",
   "metadata": {
    "id": "bca4cbba"
   },
   "source": [
    "In this notebook there are certain sections where the user s required to enter its own information which are marked as comments of the form :\n",
    "\n",
    "<b>### ADD YOUR CODE ###</b>  , so <b>###</b> can be searched to know what are those sections.\n",
    "\n",
    "All cells are assigned an ID, as a comment at the top of the cell,for example as: <b>#ID 10(5)</b>, where 10 denotes the cell ID and 5 denotes the Section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ge7C0gfOXtkJ",
   "metadata": {
    "id": "Ge7C0gfOXtkJ"
   },
   "source": [
    "# **1. DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714d531",
   "metadata": {
    "id": "7714d531"
   },
   "source": [
    "First of all, we will have a look at the Datasets that are pre-installed in OAB which can be used for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c45b98e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1638844258832,
     "user": {
      "displayName": "Darpan Malik",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIu_3rWheyXSS4yMFt3-iABkEjE1aDcGUBFtA64g=s64",
      "userId": "03155443515957215840"
     },
     "user_tz": -60
    },
    "id": "1c45b98e",
    "outputId": "534d6f45-db24-49f4-85c7-e10dd74981b2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.mnist\n",
      "1.fashion_mnist\n",
      "2.cifar10\n",
      "3.cifar100\n",
      "4.mvtec_ad_carpet\n",
      "5.mvtec_ad_grid\n",
      "6.mvtec_ad_leather\n",
      "7.mvtec_ad_tile\n",
      "8.mvtec_ad_wood\n",
      "9.mvtec_ad_bottle\n",
      "10.mvtec_ad_cable\n",
      "11.mvtec_ad_capsule\n",
      "12.mvtec_ad_hazelnut\n",
      "13.mvtec_ad_metal_nut\n",
      "14.mvtec_ad_pill\n",
      "15.mvtec_ad_screw\n",
      "16.mvtec_ad_toothbrush\n",
      "17.mvtec_ad_transistor\n",
      "18.mvtec_ad_zipper\n",
      "19.crack\n"
     ]
    }
   ],
   "source": [
    "#ID 2(1)\n",
    "\n",
    "#importing the necessary functions and internal variables \n",
    "\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "#from os.path import isfile, join\n",
    "from datetime import datetime \n",
    "from pathlib import Path\n",
    "sys.path.append('../..')             #setting the current directory path\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "#other  necessary imports\n",
    "from oab.data.classification_dataset import ClassificationDataset\n",
    "from oab.data.semisupervised import SemisupervisedAnomalyDataset\n",
    "from oab.data.abstract_classes import AnomalyDataset\n",
    "from oab.algorithms.abstract_classes import AbstractWrapperToRecipe,AbstractWrapperFromRecipe\n",
    "import yaml\n",
    "from ruamel.yaml import YAML,RoundTripRepresenter\n",
    "import numpy as np\n",
    "from oab.data.utils_image import mvtec_ad_suffixes, mvtec_ad_datasets, mvtec_ad_color_datasets, image_datasets, url_dict, reshape_dict\n",
    "\n",
    "\n",
    "lst_all_datasetnames =image_datasets\n",
    "for i,dataset in enumerate(lst_all_datasetnames):\n",
    "    print(f\"{i}.{dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5GfTylvidJUs",
   "metadata": {
    "id": "5GfTylvidJUs"
   },
   "source": [
    "\n",
    "`oab` provides a variety of image datasets that can easily be loaded, in either of the following ways: \n",
    "\n",
    "`1.` If a user is interested in using her own image dataset  loading **via local folder directory**, the following steps have to be followed: (1) Ensure that the format is readable by oab. This requires there to be a folder for the dataset with the subfolders `normal` and `anomaly`. Naturally, all normal images are in the folder `normal` and all images of anomalies in the folder `anomaly`. (2) Based on this folder structure, the dataset can be loaded.\n",
    "\n",
    "\n",
    "\"**Local folder structure - without URL usage**\" :\n",
    "```\n",
    "dataset_name\n",
    "        │\n",
    "        ├── normal\n",
    "        │    \n",
    "        └── anomaly\n",
    "``` \n",
    "\n",
    "`2.` If user's dataset is provided **via a URL**, then it would be downloaded and stored in the OAB's \"datasets\" folder, given that it is already formatted as per the required folder heirarchy, where the folder :\n",
    "\n",
    "`good`(which should not be renamed) : contains normal images whereas \n",
    "\n",
    "`anomaly_folder_n`(can be renamed):  contains anomalous images\n",
    "\n",
    "\n",
    "\n",
    "\"**Uploaded folder structure - with URL usage**\" :\n",
    "```\n",
    "dataset_name\n",
    "        │\n",
    "        ├── train\n",
    "        │   ├── good\n",
    "        │\n",
    "        │  \n",
    "        └── test\n",
    "            ├── good\n",
    "            ├── anomaly_folder_1\n",
    "            ├── ...\n",
    "            ├── ...\n",
    "            ├── anomaly_folder_2\n",
    "```\n",
    " \n",
    "Note: Alternatively, it is of course possible to load the images into `numpy` arrays and treat them as if they were tabular data. If this approach is to be followed, please look at the notebooks for tabular data.\n",
    "\n",
    "\n",
    "- Notes: Limited to 256x256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea2381",
   "metadata": {
    "id": "62ea2381"
   },
   "source": [
    "Below,we define a helper function for loading dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u-6F9ZW-D3qn",
   "metadata": {
    "id": "u-6F9ZW-D3qn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 3(1)\n",
    "\n",
    "def add_own_dataset(dataset_name: str, bw: bool =False):\n",
    "  \"\"\"\n",
    "  Helper that adds a dataset name to some internal variables used by oab.\n",
    "  This is a workaround as the functionality to load an own dataset where files \n",
    "  are stored in a directory was not yet available when the paper was submitted.\n",
    "\n",
    "  :param dataset_name: Name of the dataset\n",
    "  :param bw: If the dataset is black-and-white\n",
    "\n",
    "  :returns: the name which is to be used when loading the dataset.\n",
    "  \"\"\"\n",
    "  name_mvtec_prefix = \"mvtec_ad_\" + dataset_name\n",
    "  image_datasets.append(name_mvtec_prefix)\n",
    "  mvtec_ad_suffixes.append(dataset_name)\n",
    "  mvtec_ad_datasets.append(name_mvtec_prefix)\n",
    "  if bw:\n",
    "    mvtec_ad_bw_datasets.append(name_mvtec_prefix)\n",
    "    reshape_dict[name_mvtec_prefix]=(256,256)\n",
    "  else:\n",
    "    mvtec_ad_color_datasets.append(name_mvtec_prefix)\n",
    "    reshape_dict[name_mvtec_prefix]=(256,256,3)\n",
    "  # url is just used for downloading the dataset if not already available offline and  for reproducibility\n",
    "  # url_dict['myImageDataset']= 'ftp://<myftpserverurl>/myImageDataset.tar.xz \n",
    "\n",
    "  return name_mvtec_prefix \n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab14ce",
   "metadata": {
    "id": "baab14ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 4(1)\n",
    "#timestamp is set  for this run  \n",
    "\n",
    "time=datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "### ADD YOUR OWN BENCHMARK NAME ###\n",
    "benchmark_name=\"Paper_A\"\n",
    "\n",
    "#creating folder for this benchmark with current timestamp name to store recipes of this run\n",
    "if not os.path.exists(f\"{os.getcwd()}/{benchmark_name}/{time}\"): #creating directory for this benchhmark run for storing recipes\n",
    "    os.makedirs(f\"{os.getcwd()}/{benchmark_name}/{time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jap0jcY8JQwb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1638844275634,
     "user": {
      "displayName": "Darpan Malik",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIu_3rWheyXSS4yMFt3-iABkEjE1aDcGUBFtA64g=s64",
      "userId": "03155443515957215840"
     },
     "user_tz": -60
    },
    "id": "Jap0jcY8JQwb",
    "outputId": "0fd23f5c-993d-4aa0-c5ba-f1d32eba6513",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset(s) successfully added as :  ['mvtec_ad_myImageDataset']\n",
      "Datasets of OAB:\n",
      "0.mnist\n",
      "1.fashion_mnist\n",
      "2.cifar10\n",
      "3.cifar100\n",
      "4.mvtec_ad_carpet\n",
      "5.mvtec_ad_grid\n",
      "6.mvtec_ad_leather\n",
      "7.mvtec_ad_tile\n",
      "8.mvtec_ad_wood\n",
      "9.mvtec_ad_bottle\n",
      "10.mvtec_ad_cable\n",
      "11.mvtec_ad_capsule\n",
      "12.mvtec_ad_hazelnut\n",
      "13.mvtec_ad_metal_nut\n",
      "14.mvtec_ad_pill\n",
      "15.mvtec_ad_screw\n",
      "16.mvtec_ad_toothbrush\n",
      "17.mvtec_ad_transistor\n",
      "18.mvtec_ad_zipper\n",
      "19.crack\n",
      "20.mvtec_ad_myImageDataset\n"
     ]
    }
   ],
   "source": [
    "#ID 5(1)\n",
    "\n",
    "\n",
    "#### ADD YOUR DATASETNAME(S)  HERE ###\n",
    "\n",
    "own_datasets_list=[\"myImageDataset\"]  # More of user's own datasets can be added in this list\n",
    "oab_datasets_list=['mnist']   # More of OAB's datasets can be added to this list\n",
    "\n",
    "\n",
    "# 'myImageDataset' is the name of the Dataset(which the user loads for benchmarking as well as the name of the folder containing normal and anomaly folders\n",
    "#  which further contain the respective images\n",
    "#  and make sure folder structure is correct ({dataset_folder}/{name-without mvtec_ad_}/[normal/anomaly])\n",
    "\n",
    "dataset_folder='datasets'     #all image datasets folders are stored in this directory\n",
    " \n",
    "new_recipe_path=f\"{benchmark_name}/{time}/{time}_{benchmark_name}\" # file path for new recipe created in this run    \n",
    "    \n",
    "# calling the helper function to update internal OAB variables \n",
    "mvtec_ad_own_datasets_list=[]\n",
    "for dataset_name in own_datasets_list:\n",
    " mvtec_ad_own_datasets_list.append(add_own_dataset(dataset_name))\n",
    "print( f\" Dataset(s) successfully added as :  {mvtec_ad_own_datasets_list}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Now,  we'll have a look at all the datasets again which are pre-installed in OAB,after adding own_datasets \n",
    "lst_all_datasetnames =image_datasets\n",
    "print(\"Datasets of OAB:\")\n",
    "for i,dataset in enumerate(lst_all_datasetnames):\n",
    "    print(f\"{i}.{dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k2lQC5bsi87O",
   "metadata": {
    "id": "k2lQC5bsi87O"
   },
   "source": [
    "Now, If this dataset(s) was already stored in the dataset_folder, structured as mentioned in the initial description of this section above, then we have to create the file \"applied_modification.txt\" in Path(dataset_folder)/dataset_name/ \"applied_modification.txt\"). If this is not available in the location, then we download the dataset from the given URL(When data is downloaded for the URL, the orientation of the folders as well as the image resizing operation is performed and  information about that is stored in \"applied_modification.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sQUa14_KqO9C",
   "metadata": {
    "id": "sQUa14_KqO9C",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 6(1)\n",
    "for dataset_name in own_datasets_list:\n",
    "  open(Path(dataset_folder)/dataset_name/\"applied_modification.txt\", \"w\") \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c1bdac",
   "metadata": {
    "id": "c0c1bdac"
   },
   "source": [
    "# **2. DATA SELECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NZ9L0Cc-NPsB",
   "metadata": {
    "id": "NZ9L0Cc-NPsB"
   },
   "source": [
    "\n",
    "Datasets can either be loaded directly as anomaly datasets or as classification datasets. In the former case, the dataset is automatically fully prepared and ready for sampling. In the latter case, further preprocessing is still possible and necessary.\n",
    "\n",
    "Note that the automatic preprocessing for image datasets is to scale each value by `1/255`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11dpL-kh5qX",
   "metadata": {
    "id": "f11dpL-kh5qX"
   },
   "source": [
    "**After adding own dataset(s) in #ID 5 and #ID 6,the user is able to load own dataset(s) using this method :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BkZNi850qMy6",
   "metadata": {
    "id": "BkZNi850qMy6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ID 7(2)\n",
    "\n",
    "from oab.data.load_image_dataset import _load_image_dataset\n",
    "datasets={}  # contain dataset objects of own dataset names \n",
    "\n",
    "\n",
    "for dataset_name in mvtec_ad_own_datasets_list:  # loading own datasets \n",
    "    datasets[dataset_name[9:]]=_load_image_dataset(dataset_name ,anomaly_dataset=False,semisupervised=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jZrY4gxINSqv",
   "metadata": {
    "id": "jZrY4gxINSqv"
   },
   "source": [
    "### **2.1 Load anomaly detection datasets (with or without further preprocessing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1GoNZY8YNSu0",
   "metadata": {
    "id": "1GoNZY8YNSu0"
   },
   "source": [
    "In this section, we load some pre-installed data sets, namely the transistor dataset from MVTecAD , Cifar10 as well as MNIST datasets. This can be achieved using the `load_dataset` function. By default, it creates an anomaly dataset from which sampling is directly possible  and first create classsifcation dataset and then anomaly dataset,either with the preprocessing applied (`preprocess_classification_dataset=True`) i.e. Scaling is performed on all values by the factor of 1/255, or without (`preprocess_classification_dataset=False`, default).\n",
    "\n",
    "`In our case` we set have already imported own datasets with `anomaly_dataset=False ` and `preprocess_classification_dataset=False` in <b>#ID 7(2)</b> and we will also load the OAB datasets in the same way in <b>#ID 9(2)</b>\n",
    "\n",
    "If this is to be used for semisupervised anomaly detection, `semisupervised=True` needs to be specified.\n",
    "\n",
    "Note that as discussed in the paper, multiclass classification datasets like Cifar10 and MNIST are loaded with the class label `0` as normal label and all other labels as anomaly labels by default. (Alternatively, `oab` can automatically iterate through all classes as normal classes. This is not covered here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CD01W98INX59",
   "metadata": {
    "id": "CD01W98INX59",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 8(2)\n",
    "\n",
    "# imports\n",
    "from oab.data.load_dataset import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6VrjgVJANX8V",
   "metadata": {
    "id": "6VrjgVJANX8V",
    "outputId": "91a5f3d5-5aad-460c-86ca-81773f0a3443",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'myImageDataset': <oab.data.classification_dataset.ClassificationDataset object at 0x7fb6f83e0430>, 'mnist': <oab.data.classification_dataset.ClassificationDataset object at 0x7fb6f83e0c10>}\n"
     ]
    }
   ],
   "source": [
    "#ID 9(2)\n",
    "\n",
    "#### ADD YOUR OWN NUMBER OF DATASETS AND FROM OAB FOR BENCHMARKING  ###\n",
    "\n",
    "\n",
    "for dataset_name in oab_datasets_list:  # loading benchmark's datasets\n",
    "    datasets[dataset_name]=load_dataset(dataset_name,anomaly_dataset=False,preprocess_classification_dataset=False, semisupervised=True)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad12196",
   "metadata": {
    "id": "7ad12196"
   },
   "source": [
    "# **3. PREPROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x-q3roMEtFfJ",
   "metadata": {
    "id": "x-q3roMEtFfJ"
   },
   "source": [
    "The  resizing of images(only when dataset is downloaded using URL) and scaling of images has already been performed while loading the datasets as shown in previous the Sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Balwbv2ANnT-",
   "metadata": {
    "id": "Balwbv2ANnT-"
   },
   "source": [
    "Standard preprocessing steps like deleting columns, encoding categorical values differently, or removing missing values do not apply to image data. Therefore, these methods (as well as own preprocessing steps and how these are captured) are covered in the tabular dataset benchmarks.\n",
    "\n",
    "Here, we only show two preprocessing steps that are applied to datasets in `preprocess_datasets`(loaded in 2.2), which can also be performed individually depending upon requirement :\n",
    "- `Scale` all values by `1/255`.\n",
    "- `Transform the dataset into an anomaly dataset` for semisupervised anomaly detection by setting the class label `0` to normal and all other class labels to anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c570a6c",
   "metadata": {
    "id": "1c570a6c"
   },
   "source": [
    "Firstly, we will define  helper functions which stores dataset information in a recipe file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FhPX8wPfNmwF",
   "metadata": {
    "id": "FhPX8wPfNmwF",
    "outputId": "5b2995bd-dada-449c-89a5-653376fe420d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling performed on datasets! \n"
     ]
    }
   ],
   "source": [
    "#ID 11 (3)                            SCALING APPLIED\n",
    "\n",
    "#used imports from #ID 3                                          \n",
    "for dataset_name in datasets:\n",
    "    \n",
    "    datasets[dataset_name].scale(scaling_factor=1/255)\n",
    "    operations=datasets[dataset_name].operations_performed\n",
    "    #NEW RECIPE CREATED\n",
    "    datasets[dataset_name].write_operations_to_yaml(f\"{new_recipe_path}_{dataset_name}_preprocessing.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IZUpLqmtsyGc",
   "metadata": {
    "id": "IZUpLqmtsyGc"
   },
   "source": [
    "The individual recipe files now contains information about how to preprocess(i.e. perform scaling) the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DWm4l0mUstgE",
   "metadata": {
    "id": "DWm4l0mUstgE",
    "outputId": "5519f8cf-7f05-4a88-8c53-75382ee30475",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Paper_A/20211207031059/20211207031059_Paper_A_myImageDataset_preprocessing.yaml\n",
      "\n",
      "standard_functions:\n",
      "- name: scale\n",
      "  parameters:\n",
      "    scaling_factor: 0.00392156862745098\n",
      "2.Paper_A/20211207031059/20211207031059_Paper_A_mnist_preprocessing.yaml\n",
      "\n",
      "standard_functions:\n",
      "- name: scale\n",
      "  parameters:\n",
      "    scaling_factor: 0.00392156862745098\n"
     ]
    }
   ],
   "source": [
    "#ID 12(3)\n",
    "\n",
    "for (i,dataset_name) in enumerate(datasets):\n",
    "    print(f\"{i+1}.{new_recipe_path}_{dataset_name}_preprocessing.yaml\\n\")\n",
    "    !cat {new_recipe_path}_{dataset_name}_preprocessing.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZSTGiI5Qsi6e",
   "metadata": {
    "id": "ZSTGiI5Qsi6e",
    "outputId": "50843058-3ab8-4884-e488-26cb98dfeaac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets after adding anomaly-conversion datasets: \n",
      "{'myImageDataset': <oab.data.semisupervised.SemisupervisedAnomalyDataset object at 0x7fb63666b2e0>, 'mnist': <oab.data.semisupervised.SemisupervisedAnomalyDataset object at 0x7fb6366459a0>}\n"
     ]
    }
   ],
   "source": [
    "#ID 13(3)                            ANOMALY-DATASET CONVERSION PERFORMED\n",
    "\n",
    "#used import from #ID 2 \n",
    "\n",
    "\n",
    "#recipe_path=f\"{benchmark_name}/{time}_{benchmark_name}_recipe.yaml\"                                           \n",
    "\n",
    "datasets_ad={}    \n",
    "    # for storing dataset objects converted to anomaly-dataset\n",
    "for dataset_name in datasets:   \n",
    "     datasets_ad[dataset_name]= SemisupervisedAnomalyDataset(classification_dataset=datasets[dataset_name],\n",
    "                                                       normal_labels=0,\n",
    "                                                       yamlpath_append=f\"{new_recipe_path}_{dataset_name}_preprocessing.yaml\")   \n",
    "     \n",
    "        \n",
    "                                                                            \n",
    "print(\"datasets after adding anomaly-conversion datasets: \")    \n",
    "print(datasets_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4HSf5lCJczbu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HSf5lCJczbu",
    "outputId": "8573e270-eb3c-4c2e-e399-084b104a21b9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Paper_A/20211207031059/20211207031059_Paper_A_myImageDataset_preprocessing.yaml\n",
      "\n",
      "standard_functions:\n",
      "- name: scale\n",
      "  parameters:\n",
      "    scaling_factor: 0.00392156862745098\n",
      "anomaly_dataset:\n",
      "  arguments:\n",
      "    normal_labels: 0\n",
      "    anomaly_labels:\n",
      "2.Paper_A/20211207031059/20211207031059_Paper_A_mnist_preprocessing.yaml\n",
      "\n",
      "standard_functions:\n",
      "- name: scale\n",
      "  parameters:\n",
      "    scaling_factor: 0.00392156862745098\n",
      "anomaly_dataset:\n",
      "  arguments:\n",
      "    normal_labels: 0\n",
      "    anomaly_labels:\n"
     ]
    }
   ],
   "source": [
    "#ID 14(3)\n",
    "\n",
    "for (i,dataset_name) in enumerate(datasets):\n",
    "    print(f\"{i+1}.{new_recipe_path}_{dataset_name}_preprocessing.yaml\\n\")\n",
    "    !cat {new_recipe_path}_{dataset_name}_preprocessing.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f600e5a",
   "metadata": {
    "id": "4f600e5a"
   },
   "source": [
    "# **4. SAMPLING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f817a6ab",
   "metadata": {
    "id": "f817a6ab"
   },
   "source": [
    "Here, we define the sampling parameters to sample from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae62b9",
   "metadata": {
    "id": "6bae62b9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 15(4)\n",
    "\n",
    "### ADD YOUR OWN SAMPLING PARAMETERS ###\n",
    "\n",
    "# sampling parameters\n",
    "training_split = 0.7                 # Specifies the proportion of normal data points that will be used during training\n",
    "max_contamination_rate = 0.5         # Maximum contamination rate of the test set. If this is exceeded, not all anomalies that exist are sampled\n",
    "n_steps = 1        # n_steps=10      # Number of samples to be taken\n",
    "\n",
    "#These below are the possible sampling types to sample from datasets\n",
    "sampling_types=['semisupervised_multiple','semisupervised_explicit_numbers_single','semisupervised_training_split_multiple','semisupervised_training_split_single']\n",
    "\n",
    "\n",
    "sampling_type='semisupervised_training_split_multiple'  #by default for this run\n",
    "\n",
    "sampling_params={'training_split':training_split,'max_contamination_rate':max_contamination_rate,'n_steps':n_steps,'flatten_images':False} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZpZe-6r6tdIl",
   "metadata": {
    "id": "ZpZe-6r6tdIl"
   },
   "source": [
    "The above sampling parameters are utilized in\n",
    "<b>#ID 27(5) </b> \n",
    "for sampling the datasets(except for the pre-installed mv_tec_ad_datasets) before training the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49db16",
   "metadata": {
    "id": "3a49db16",
    "outputId": "629a165e-d053-4933-ef7e-90cb6c371bff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'myImageDataset': <oab.data.semisupervised.SemisupervisedAnomalyDataset object at 0x7fb63666b2e0>, 'mnist': <oab.data.semisupervised.SemisupervisedAnomalyDataset object at 0x7fb6366459a0>}\n"
     ]
    }
   ],
   "source": [
    "#ID 16(4)\n",
    "\n",
    "# Adding own and oab's {dataset_name:dataset_object} pair to benchmarking_datasets \n",
    "benchmarking_datasets={}\n",
    "\n",
    "for (x,y) in datasets_ad.items():\n",
    "    benchmarking_datasets[x]=y\n",
    "\n",
    "print(benchmarking_datasets)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248c3ad",
   "metadata": {
    "id": "e248c3ad"
   },
   "source": [
    "This dictionary <b>benchmarking_datasets</b> will be used for the Benchmarking as it contains all the information:\n",
    "    \n",
    "    \n",
    "    1.dataset_name\n",
    "    2.final_dataset_object(preprocessed and anomaly-converted)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd1b0f",
   "metadata": {
    "id": "babd1b0f"
   },
   "source": [
    "\n",
    "\n",
    "# **5. ALGORITHM TRAINING AND TESTING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tjmqC3lBaMwX",
   "metadata": {
    "id": "tjmqC3lBaMwX"
   },
   "source": [
    "We first download and import algorithms used for anomaly decection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OZFq2XBfaMRN",
   "metadata": {
    "id": "OZFq2XBfaMRN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 17(5)\n",
    "\n",
    "import wget\n",
    "\n",
    "wget.download('https://raw.githubusercontent.com/ISDM-CAU-Kiel/oab/master/notebooks/benchmark_image/cae_ocsvm.py', \"cae_ocsvm.py\")\n",
    "wget.download('https://raw.githubusercontent.com/ISDM-CAU-Kiel/oab/master/notebooks/benchmark_image/cae_iforest.py', \"cae_iforest.py\")\n",
    "wget.download('https://raw.githubusercontent.com/ISDM-CAU-Kiel/oab/master/notebooks/benchmark_image/conv_ae.py', \"conv_ae.py\")\n",
    "### ADD your algo import here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NmVNOM7Uak1i",
   "metadata": {
    "id": "NmVNOM7Uak1i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 18(5)\n",
    "\n",
    "# used imports from #ID 2\n",
    "# used imports from #ID 8\n",
    "\n",
    "\n",
    "## ADD your algo(s) import here ###\n",
    "#from module_name import class_name\n",
    "\n",
    "from conv_ae import ConvAutoEncoder \n",
    "from cae_ocsvm import CAEOCSVM\n",
    "from cae_iforest import CAEIForest\n",
    "from oab.evaluation import EvaluationObject, ComparisonObject,all_metrics\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637590bb",
   "metadata": {
    "id": "637590bb"
   },
   "source": [
    "Firstly, we define hyperparameters for all algorithms and choose for benchmarking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef70b70",
   "metadata": {
    "id": "eef70b70",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 19(5)\n",
    "\n",
    "   \n",
    "### Extend Algos dictionary with own algorithm specifications as shown below for OAB algorithms ###   \n",
    "\n",
    "### ADD YOUR OWN (HYPER)PARAMETERS AND THEIR VALUES FOR PRE-INSTALLED ALGOS###\n",
    "\n",
    "\n",
    "lst_benchmark_algorithms = [\n",
    "    \n",
    "    {\n",
    "       \"algo_module_name\": \"cae_iforest\",   \n",
    "       \"algo_class_name\": \"CAEIForest\",\n",
    "       \"algo_name_in_result_table\": \"CAE v2\",\n",
    "       \"algo_parameters\": {\"CAE_parameters\": {'latent_dim': 100, 'epochs': 50, 'verbose': 0}, \"IForest_parameters\": {'random_state': 42} },\n",
    "        \"fit\": {'method_name': 'fit', 'params': {}}, \n",
    "        \"decision_function\": {'method_name': 'decision_function', 'params': {}}\n",
    "        }]\n",
    "'''    \n",
    "      ### ConvAutoEncoder is not used, uncomment to use for benchmarking\n",
    "      {\n",
    "       \"algo_module_name\": \"conv_ae\"   ,\n",
    "       \"algo_class_name\": \"ConvAutoEncoder\",\n",
    "       \"algo_name_in_result_table\": \"CAE v1\",\n",
    "       \"algo_parameters\":   {'latent_dim': 100, 'epochs': 50, 'verbose': 0},\n",
    "        \"fit\": {'method_name': 'fit', 'params': {}}, \n",
    "       \"decision_function\": {'method_name': 'decision_function', 'params': {}}\n",
    "       }  ]\n",
    " \n",
    " ### CAEOIforest is not used, uncomment ###    \n",
    "       ]\n",
    "\n",
    "\n",
    "\n",
    "### CAEOCSVM is not used, uncomment ### \n",
    "    {   \n",
    "       \"algo_module_name\": \"cae_ocsvm\" , \n",
    "       \"algo_class_name\": \"CAEOCSVM\",\n",
    "       \"algo_name_in_result_table\": \"CAE v3\",\n",
    "       \"algo_parameters\": {\"CAE_parameters\": {'latent_dim': 100, 'epochs': 50, 'verbose': 0},\"OCSVM_parameters\": {'degree': 3}},\n",
    "       \"fit\": {'method_name': 'fit', 'params': {}}, \n",
    "       \"decision_function\": {'method_name': 'decision_function', 'params': {}}\n",
    "    } \n",
    "]    \n",
    "'''      \n",
    "  \n",
    "\n",
    "own_algorithms=[]  #add your own algorithm dictionaries like above for pre-installed algos\n",
    "\n",
    "lst_benchmark_algorithms.extend(own_algorithms)\n",
    "\n",
    "\n",
    "#seed value defined for ths benchmark run for obtaining consistent results \n",
    "seed=42 # seeting seed value for consistency in reproducibility of benchmark   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de5187",
   "metadata": {
    "id": "00de5187"
   },
   "source": [
    "<b>LOAD YOUR RECIPES</b> to be repdroduced and use it in the current benchmark run, lets,see  <b>how it looks</b>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b5430",
   "metadata": {
    "id": "b19b5430",
    "outputId": "744292d1-9278-4be2-9575-3a98d5f510f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.conv_ae\n",
      "\n",
      "decision_function:\r\n",
      "  method_name: decision_function\r\n",
      "  params: {}\r\n",
      "fit:\r\n",
      "  method_name: fit\r\n",
      "  params: {}\r\n",
      "init:\r\n",
      "  params:\r\n",
      "    epochs: 50\r\n",
      "    latent_dim: 100\r\n",
      "    verbose: 0\r\n",
      "seed: 42    \r\n"
     ]
    }
   ],
   "source": [
    "#ID 21(5)\n",
    "recipe_files_algos={'conv_ae': \"Paper_B/202120211206212223_Paper_B_conv_ae[ConvAutoEncoder]_recipe.yaml\"} #add recipe files to this dictionary\n",
    "\n",
    "for (i,file) in enumerate(recipe_files_algos):\n",
    "    print(f\"{i+1}.{file}\\n\")\n",
    "    !cat {recipe_files_algos[file]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38327e77",
   "metadata": {
    "id": "38327e77",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ID 20(5)\n",
    "recipe_files_datasets={} #add recipe files to this dictionary\n",
    "\n",
    "for (i,file) in enumerate(recipe_files_datasets):\n",
    "    print(f\"{i+1}.{file}\\n\")\n",
    "    !cat {recipe_files_datasets[file]} \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef08908",
   "metadata": {
    "id": "4ef08908",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 22(5)\n",
    "#Loading datasets from recipes wth preprocessing and anomaly-conversion applied\n",
    "\n",
    "for dataset_name in recipe_files_datasets:  # loading benchmark's datasets\n",
    "    cd=load_dataset(dataset_name,anomaly_dataset=False,preprocess_classification_dataset=False, semisupervised=True)\n",
    "    cd.perform_operations_from_yaml(filepath=recipe_files_datasets[dataset_name]) #performing preprocess operations\n",
    "    ad=cd.tranform_from_yaml(filepath=recipe_files_datasets[dataset_name],semisupervised=True) #anomaly-dataset conversion\n",
    "    benchmarking_datasets[dataset_name]=ad   # dding datasets from recipe to benchmarking_datasets\n",
    "    \n",
    "    #creating new preprocessing recipe from Paper_B to  Paper_A's(i.e. who runs this benchhmark ) folder\n",
    "    yaml=YAML(typ='rt')\n",
    "    yaml_content = yaml.load(Path(\"./\") / recipe_files_datasets[dataset_name])\n",
    "    yaml.dump(yaml_content,Path(\"./\")/f\"{new_recipe_path}_{dataset_name}_preprocessing.yaml\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e8a20",
   "metadata": {
    "id": "094e8a20",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 23(5)\n",
    "#Loading Algorithms from recipes and using it for benchmarking\n",
    "\n",
    "import re\n",
    "from json import loads,dumps\n",
    "               \n",
    "for algo_name in recipe_files_algos:\n",
    "    yaml=YAML(typ='rt')\n",
    "    yaml_content = yaml.load(Path(\"./\") / recipe_files_algos[algo_name])\n",
    "    class_name=re.findall(r'\\[.*?\\]',recipe_files_algos[algo_name]) # get class_name from filename of algo   \n",
    "    recipe_algo={   \n",
    "       \"algo_module_name\": algo_name , \n",
    "       \"algo_class_name\": class_name[0][1:-1],\n",
    "       \"algo_name_in_result_table\": algo_name ,\n",
    "       \"algo_parameters\":loads(dumps(yaml_content['init']['params'])),\n",
    "       \"fit\":loads(dumps(yaml_content['fit'])), \n",
    "       \"decision_function\": loads(dumps(yaml_content['decision_function'])),\n",
    "    }\n",
    "    seed=loads(dumps(yaml_content['seed']))          # setting seed value from recipe\n",
    "    if recipe_algo not in lst_benchmark_algorithms:   \n",
    "      lst_benchmark_algorithms.append(recipe_algo)\n",
    "      # append recipe algo in benchmarking algos for this run\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb76fd8",
   "metadata": {
    "id": "1eb76fd8"
   },
   "source": [
    "Now, For every benchmark dataset , we sample from that dataset to train the algorithms and then predict the outcomes for each dataset with each algortihm and then store results in a evaluation object, which is then added to the comparison object to show the final Benchmarking results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902ee01",
   "metadata": {
    "id": "0902ee01"
   },
   "source": [
    "For benchmark run, there are a couple of minor <b>bug fixes</b>  found in the OAB modules, which are dealt with below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0036424e",
   "metadata": {
    "id": "0036424e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 24(5)\n",
    "\n",
    "#               1.-------------------------------\n",
    "# import manually (and due to a missing import, assign into correct module)\n",
    "from oab.data.utils import _make_yaml,_append_to_yaml,_append_sampling_to_yaml\n",
    "from oab.data import semisupervised\n",
    "semisupervised._append_sampling_to_yaml = _append_sampling_to_yaml\n",
    "\n",
    "\n",
    "#               2.--------------------------------\n",
    "#from oab.algorithms.semisupervised_wrapper import SemisupervisedWrapperToRecipe (syntactical error in class)\n",
    "from typing import Dict\n",
    "class SemisupervisedWrapperToRecipe(AbstractWrapperToRecipe):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def track_init(self, obj, params: Dict = {}):\n",
    "        self.init_dict = {'params': params}\n",
    "        return obj(**params)\n",
    "\n",
    "    def track_fit(self, x, obj, params: Dict = {}, fit_method: str = 'fit'):\n",
    "        self.fit_dict = {\n",
    "            'method_name': fit_method,\n",
    "            'params': params\n",
    "        }\n",
    "        return getattr(obj, fit_method)(x, **params)\n",
    "\n",
    "    def track_decision_function(self, x ,obj, params: Dict = {},\n",
    "        decision_function_method: str = 'decision_function'):\n",
    "        self.decision_function_dict = {\n",
    "            'method_name': decision_function_method,\n",
    "            'params': params\n",
    "        }\n",
    "        return getattr(obj, decision_function_method)(x, **params)\n",
    "\n",
    "    \n",
    "    def store_recipe(self, yaml_path):\n",
    "            yaml_content = {\n",
    "                'init': self.init_dict,\n",
    "                'fit': self.fit_dict,\n",
    "                'decision_function': self.decision_function_dict\n",
    "            }\n",
    "            with open(yaml_path, \"w+\") as stream:\n",
    "                 yaml.dump(yaml_content, stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ea255",
   "metadata": {
    "id": "979ea255"
   },
   "source": [
    "We also define the helper function for performing sampling depending on choice of the user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f60f20",
   "metadata": {
    "id": "c4f60f20",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 25(5)\n",
    "def sample_using_recipe(dataset_name,sampling_type,sampling_params):    \n",
    "    \n",
    "    if sampling_type == 'semisupervised_multiple':\n",
    "      return benchmarking_datasets[dataset_name].sample_multiple(**sampling_params,yamlpath_append=f\"{new_recipe_path}_{dataset_name}_preprocessing.yaml\")\n",
    "    elif sampling_type == 'semisupervised_explicit_numbers_single':\n",
    "      return benchmarking_datasets[dataset_name].sample_with_explicit_numbers(**sampling_params,yamlpath_append=f\"{new_recipe_path}_{dataset_name}_preprocessing.yaml\")  \n",
    "    elif sampling_type == 'semisupervised_training_split_multiple':\n",
    "      return benchmarking_datasets[dataset_name].sample_multiple_with_training_split(**sampling_params,yamlpath_append=f\"{new_recipe_path}_{dataset_name}_preprocessing.yaml\")  \n",
    "    elif sampling_type == 'semisupervised_training_split_single':        \n",
    "      return benchmarking_datasets[dataset_name].sample_with_training_split(**sampling_params,yamlpath_append=f\"{new_recipe_path}_{dataset_name}_preprocessing.yaml\")   \n",
    "    else:\n",
    "        raise NotImplementedError(f\"Sampling from yaml with type {type} is not implemented.\")      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b9703",
   "metadata": {
    "id": "7a9b9703",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 26(5)\n",
    "#-----------ALL ALGOS AND DATASETS FOR BENCHMARK RUN---------#\n",
    "\n",
    "\n",
    "print(\"Datasets obtained from recipe:\")    \n",
    "for dataset in recipe_files_datasets:\n",
    "    print(dataset_name)\n",
    "#print(f\"recipe_datasets:{recipe_datasets}\")\n",
    "#print(f\"benchmarking_datasets: {benchmarking_datasets}\")      \n",
    "\n",
    "#print(\"\\n\") \n",
    "#print(\"Algos obtained from recipe:\")    \n",
    "for algo_name in recipe_files_algos:\n",
    "    print(algo_name)\n",
    "#print(\"\\n\")    \n",
    "#print(\"ALl Datasets for this benchmark run:\")    \n",
    "for dataset_name in benchmarking_datasets:\n",
    "    print(dataset_name)\n",
    "#print(f\"recipe_datasets:{    \n",
    "#print(\"\\n\") \n",
    "#print(\"All algos for this benchmark run:\")\n",
    "for algo in lst_benchmark_algorithms:\n",
    "    print(algo['algo_module_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763aeb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0763aeb2",
    "outputId": "5b15015f-6bb6-4d74-f7ea-5ba4e7082c91",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------myImageDataset-------\n",
      "------cae_iforest\n",
      ".Evaluation on dataset mvtec_ad_myImageDataset with normal labels [0] and anomaly labels [1.0].\n",
      "Total of 1 datasets. Per dataset:\n",
      "89 training instances, 57 test instances, training contamination rate 0.0, test contamination rate 0.3157894736842105.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.786 \t 0.000 \t\t roc_auc\n",
      "0.540 \t 0.000 \t\t adjusted_average_precision\n",
      "0.675 \t 0.000 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "------conv_ae\n",
      ".Evaluation on dataset mvtec_ad_myImageDataset with normal labels [0] and anomaly labels [1.0].\n",
      "Total of 1 datasets. Per dataset:\n",
      "89 training instances, 57 test instances, training contamination rate 0.0, test contamination rate 0.3157894736842105.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.786 \t 0.000 \t\t roc_auc\n",
      "0.481 \t 0.000 \t\t adjusted_average_precision\n",
      "0.634 \t 0.000 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "-------mnist-------\n",
      "------cae_iforest\n",
      ".Evaluation on dataset mnist with normal labels [0] and anomaly labels [1, 2, 3, 4, 5, 6, 7, 8, 9].\n",
      "Total of 1 datasets. Per dataset:\n",
      "4832 training instances, 4142 test instances, training contamination rate 0.0, test contamination rate 0.5.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.982 \t 0.000 \t\t roc_auc\n",
      "0.961 \t 0.000 \t\t adjusted_average_precision\n",
      "0.980 \t 0.000 \t\t precision_recall_auc\n",
      "\n",
      "\n",
      "------conv_ae\n",
      ".Evaluation on dataset mnist with normal labels [0] and anomaly labels [1, 2, 3, 4, 5, 6, 7, 8, 9].\n",
      "Total of 1 datasets. Per dataset:\n",
      "4832 training instances, 4142 test instances, training contamination rate 0.0, test contamination rate 0.5.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.935 \t 0.000 \t\t roc_auc\n",
      "0.832 \t 0.000 \t\t adjusted_average_precision\n",
      "0.916 \t 0.000 \t\t precision_recall_auc\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ID 27(5)                # RUNNING THE BENCHMARK\n",
    "co = ComparisonObject()\n",
    "for dataset_name in benchmarking_datasets:\n",
    "    print(f'-------{dataset_name}-------') \n",
    "    \n",
    "    #print(mvtec_ad_own_datasets_list)\n",
    "    for alg in lst_benchmark_algorithms:\n",
    "        name=alg[\"algo_module_name\"]\n",
    "        print(f\"------{name}\")\n",
    "        eval_obj = EvaluationObject(algorithm_name=alg[\"algo_name_in_result_table\"])\n",
    "        algo_parameters=alg['algo_parameters']\n",
    "        algo= getattr(__import__(alg[\"algo_module_name\"]),alg[\"algo_class_name\"]) # Algo object imported from class \n",
    "        algo_initialized=algo(**algo_parameters)\n",
    "        seed=42\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed) \n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        if dataset_name[:9]=='mvtec_ad_' :  # if dataset is an 'mv_tec' dataset\n",
    "            (x_train, x_test, y_test), sample_config in benchmarking_datasets[dataset_name].sample_original_mvtec_split(flatten_images=False)\n",
    "            w = SemisupervisedWrapperToRecipe()\n",
    "            w.track_init(algo, params=algo_parameters)\n",
    "            w.track_fit(x=x_train, obj=algo_initialized, params=alg['fit']['params'], fit_method=alg['fit']['method_name']) # the last parameter is the name of the method used for fitting\n",
    "            pred = w.track_decision_function(x_test,algo_initialized,params=alg['decision_function']['params'], decision_function_method=alg['decision_function']['method_name']) # the last parameter is the field name used to store anomaly scores by the model\n",
    "            w.store_recipe(f\"{new_recipe_path}_{name}_recipe.yaml\")\n",
    "            eval_obj.add(ground_truth=y_test, prediction=pred, description=sample_config)  \n",
    "                    \n",
    "            \n",
    "        else:\n",
    "    \n",
    "         if dataset_name in recipe_files_datasets:\n",
    "          for (x_train, x_test, y_test), sample_config in benchmarking_datasets[dataset_name].sample_from_yaml(recipe_files_datasets[dataset_name],type=sampling_type):\n",
    "            print('.', end='') # update to see progress \n",
    "            w = SemisupervisedWrapperToRecipe()\n",
    "            w.track_init(algo, params=algo_parameters)\n",
    "            w.track_fit(x=x_train, obj=algo_initialized, params=alg['fit']['params'], fit_method=alg['fit']['method_name']) # the last parameter is the name of the method used for fitting\n",
    "            pred = w.track_decision_function(x_test,algo_initialized,params=alg['decision_function']['params'], decision_function_method=alg['decision_function']['method_name']) # the last parameter is the field name used to store anomaly scores by the model\n",
    "            w.store_recipe(f\"{new_recipe_path}_{name}_recipe.yaml\")\n",
    "            eval_obj.add(ground_truth=y_test, prediction=pred, description=sample_config)\n",
    "        \n",
    "         else:\n",
    "          \n",
    "          for (x_train, x_test, y_test), sample_config in benchmarking_datasets[dataset_name].sample_using_recipe(dataset_name,sampling_type,sampling_params):\n",
    "            print('.', end='') # update to see progress \n",
    "            w = SemisupervisedWrapperToRecipe()\n",
    "            w.track_init(algo, params=algo_parameters)\n",
    "            w.track_fit(x=x_train, obj=algo_initialized, params=alg['fit']['params'], fit_method=alg['fit']['method_name']) # the last parameter is the name of the method used for fitting\n",
    "            pred = w.track_decision_function(x_test,algo_initialized,params=alg['decision_function']['params'] , decision_function_method=alg['decision_function']['method_name']) # the last parameter is the field name used to store anomaly scores by the model\n",
    "            w.store_recipe(f\"{new_recipe_path}_{name}_recipe.yaml\")\n",
    "            _append_to_yaml(f\"{new_recipe_path}_{name}_recipe.yaml\",'seed',seed)\n",
    "            eval_obj.add(ground_truth=y_test, prediction=pred, description=sample_config)  \n",
    "               \n",
    "            \n",
    "            \n",
    "        eval_desc = eval_obj.evaluate(print=True,metrics=['roc_auc', 'adjusted_average_precision', 'precision_recall_auc'])\n",
    "        co.add_evaluation(eval_desc)\n",
    "        print(\"\\n\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c585e",
   "metadata": {
    "id": "501c585e"
   },
   "source": [
    "# **6. EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57beebf",
   "metadata": {
    "id": "e57beebf"
   },
   "source": [
    "Here , we will see how different metrics can be selected when evaluating an algorithm's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05bc232",
   "metadata": {
    "id": "d05bc232"
   },
   "source": [
    "In previous section while creating an evalutation description,  we used all metrics for evaluation:\n",
    "\n",
    "     eval_desc = eval_obj.evaluate(print=False, metrics=all_metrics)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97869e2",
   "metadata": {
    "id": "f97869e2",
    "outputId": "6da4ab4d-e5b3-44e5-ab10-93dd0a4008c7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roc_auc', 'average_precision', 'adjusted_average_precision', 'precision_n', 'adjusted_precision_n', 'precision_recall_auc']\n"
     ]
    }
   ],
   "source": [
    "#ID 28(6)\n",
    "\n",
    "# to use a subset, first see which ones are available\n",
    "\n",
    "print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d570c",
   "metadata": {
    "id": "c32d570c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ID 29(6)\n",
    "\n",
    "#### ADD YOUR OWN NUMBER OF METRICS ###\n",
    "\n",
    "#Then we can  select an arbitrary subset\n",
    "metrics=['roc_auc', 'precision_recall_auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39507d64",
   "metadata": {
    "id": "39507d64"
   },
   "source": [
    "# **7. SHOW BENCHMARK RESULTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97932df",
   "metadata": {
    "id": "c97932df"
   },
   "source": [
    "We compare by printing, the results of the evaluations of different Algo-Dataset combinations.\n",
    "\n",
    "\\[Latex version: bold for highest, italics for second highest, ?\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a61a8",
   "metadata": {
    "id": "6e3a61a8",
    "outputId": "dcc9310f-1b18-4bdb-ea5f-1781c173fbc5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "         mvtec_ad_myImageDataset     mnist   Average\n",
      "CAE v2                  0.786325  0.982190  0.884257\n",
      "conv_ae                 0.786325  0.935276  0.860801\n",
      "Average                 0.786325  0.958733       NaN\n",
      "For adjusted_average_precision:\n",
      "         mvtec_ad_myImageDataset     mnist   Average\n",
      "CAE v2                  0.540460  0.960543  0.750501\n",
      "conv_ae                 0.480902  0.832317  0.656609\n",
      "Average                 0.510681  0.896430       NaN\n",
      "For precision_recall_auc:\n",
      "         mvtec_ad_myImageDataset     mnist   Average\n",
      "CAE v2                  0.675378  0.980264  0.827821\n",
      "conv_ae                 0.633678  0.916091  0.774884\n",
      "Average                 0.654528  0.948177       NaN\n"
     ]
    }
   ],
   "source": [
    "#ID 30(7)\n",
    "\n",
    "# print results in easily readable format\n",
    "co.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b0f46",
   "metadata": {
    "id": "6f3b0f46",
    "outputId": "9048079a-54ae-4b3a-9b16-859e3dcd8c37",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "        mvtec_ad_myImageDataset         mnist   Average\n",
      "CAE v2             0.786+-0.000  0.982+-0.000  0.884257\n",
      "conv_ae            0.786+-0.000  0.935+-0.000  0.860801\n",
      "Average                   0.786         0.959       NaN\n",
      "\n",
      "For adjusted_average_precision:\n",
      "        mvtec_ad_myImageDataset         mnist   Average\n",
      "CAE v2             0.540+-0.000  0.961+-0.000  0.750501\n",
      "conv_ae            0.481+-0.000  0.832+-0.000  0.656609\n",
      "Average                   0.511         0.896       NaN\n",
      "\n",
      "For precision_recall_auc:\n",
      "        mvtec_ad_myImageDataset         mnist   Average\n",
      "CAE v2             0.675+-0.000  0.980+-0.000  0.827821\n",
      "conv_ae            0.634+-0.000  0.916+-0.000  0.774884\n",
      "Average                   0.655         0.948       NaN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ID 31(7)\n",
    "\n",
    "# print results in easily readable format with standard deviations\n",
    "co.print_results(include_stdevs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sZfxpO-5UPfp",
   "metadata": {
    "id": "sZfxpO-5UPfp",
    "outputId": "bb5ec995-2aa4-42b9-d924-bc94e4893050",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "\\begin{center}\n",
      "\\begin{tabular}{  c c c c  }\n",
      "  & mvtec\\_ad\\_myImageDataset & mnist & Average \\\\\n",
      "  CAE v2 & \\textbf{0.786$\\pm$0.000} & \\textbf{0.982$\\pm$0.000} & \\textbf{0.884} \\\\\n",
      "  conv\\_ae & \\textit{0.786$\\pm$0.000} & \\textit{0.935$\\pm$0.000} & \\textit{0.861} \\\\\n",
      "  Average & 0.786 & 0.959 &    \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "For adjusted_average_precision:\n",
      "\\begin{center}\n",
      "\\begin{tabular}{  c c c c  }\n",
      "  & mvtec\\_ad\\_myImageDataset & mnist & Average \\\\\n",
      "  CAE v2 & \\textbf{0.540$\\pm$0.000} & \\textbf{0.961$\\pm$0.000} & \\textbf{0.751} \\\\\n",
      "  conv\\_ae & \\textit{0.481$\\pm$0.000} & \\textit{0.832$\\pm$0.000} & \\textit{0.657} \\\\\n",
      "  Average & 0.511 & 0.896 &    \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "For precision_recall_auc:\n",
      "\\begin{center}\n",
      "\\begin{tabular}{  c c c c  }\n",
      "  & mvtec\\_ad\\_myImageDataset & mnist & Average \\\\\n",
      "  CAE v2 & \\textbf{0.675$\\pm$0.000} & \\textbf{0.980$\\pm$0.000} & \\textbf{0.828} \\\\\n",
      "  conv\\_ae & \\textit{0.634$\\pm$0.000} & \\textit{0.916$\\pm$0.000} & \\textit{0.775} \\\\\n",
      "  Average & 0.655 & 0.948 &    \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ID 32(7)\n",
    "\n",
    "co.print_latex(include_stdevs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ZlXKXx64aAl",
   "metadata": {
    "id": "1ZlXKXx64aAl"
   },
   "source": [
    "# **8. REPRODUCIBILITY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eBbV_ZNYiiC",
   "metadata": {
    "id": "7eBbV_ZNYiiC"
   },
   "source": [
    " ## **8.1 Creating recipes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zWTe3BI5Gfyz",
   "metadata": {
    "id": "zWTe3BI5Gfyz"
   },
   "source": [
    "This section shows **how `oab` can be used to make sampling results easily reproducible** .\n",
    " \n",
    "\n",
    "`yaml` files play an integral role in making reproducibility work, as they store the operations and parameters performed on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c0a73",
   "metadata": {
    "id": "dc4c0a73"
   },
   "source": [
    "We will see how to produce a recipe(.yaml) of the Benchmarkrun already performed  in <b>#ID 24(5)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827f5fb",
   "metadata": {
    "id": "e827f5fb"
   },
   "source": [
    "In <b>#ID 11(3) #ID 13(3) and #ID 22(5) </b>,  We already performed `creating_recipe` operations on </b>own datasets and OAB's datasets as input recipe's datasets,we can see below  the structure of those recipes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c5cf3",
   "metadata": {
    "id": "ff4c5cf3",
    "outputId": "26df1d1b-b782-4908-ee1b-87b76008a503",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Paper_A/20211207020855/20211207020855_Paper_A_cifar10_preprocessing.yaml\n",
      "\n",
      "standard_functions:\n",
      "- name: scale\n",
      "  parameters:\n",
      "    scaling_factor: 0.00392156862745098\n",
      "anomaly_dataset:\n",
      "  arguments:\n",
      "    normal_labels: 0\n",
      "    anomaly_labels:\n",
      "sampling:\n",
      "  semisupervised_training_split_multiple:\n",
      "    training_split: 0.7\n",
      "    max_contamination_rate: 0.5\n",
      "    n_steps: 1\n",
      "    random_seed: 42\n",
      "    apply_random_seed: true\n",
      "    keep_frequency_ratio_normals: false\n",
      "    equal_frequency_normals: false\n",
      "    keep_frequency_ratio_anomalies: false\n",
      "    equal_frequency_anomalies: false\n",
      "    flatten_images: false\n"
     ]
    }
   ],
   "source": [
    "#ID 33(8)\n",
    "\n",
    "for (i,dataset_name) in enumerate(benchmarking_datasets):\n",
    "    print(f\"{i+1}.{new_recipe_path}_{dataset_name}_preprocessing.yaml\\n\")\n",
    "    !cat {new_recipe_path}_{dataset_name}_preprocessing.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d244f",
   "metadata": {
    "id": "e40d244f"
   },
   "source": [
    "Also, the algorithms which were run in <b>#ID 27(5)</b> whether from OAB , user as well as from input recipe are also stored in  new recipes, their recipes have been created in the same step of #ID 27(5), as we can see below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e58c8",
   "metadata": {
    "id": "812e58c8",
    "outputId": "7c251148-593e-40aa-ec99-7ec879541a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Paper_A/20211207020855/20211207020855_Paper_A_cae_iforest_recipe.yaml\n",
      "\n",
      "init:\n",
      "  params:\n",
      "    CAE_parameters:\n",
      "      latent_dim: 100\n",
      "      epochs: 50\n",
      "      verbose: 0\n",
      "    IForest_parameters:\n",
      "      random_state: 42\n",
      "fit:\n",
      "  method_name: fit\n",
      "  params: {}\n",
      "decision_function:\n",
      "  method_name: decision_function\n",
      "  params: {}\n",
      "seed: 42\n",
      "2.Paper_A/20211207020855/20211207020855_Paper_A_conv_ae_recipe.yaml\n",
      "\n",
      "init:\n",
      "  params:\n",
      "    epochs: 50\n",
      "    latent_dim: 100\n",
      "    verbose: 0\n",
      "fit:\n",
      "  method_name: fit\n",
      "  params: {}\n",
      "decision_function:\n",
      "  method_name: decision_function\n",
      "  params: {}\n",
      "seed: 42\n"
     ]
    }
   ],
   "source": [
    "#ID 34(8)\n",
    "for (i,alg) in enumerate(lst_benchmark_algorithms):\n",
    "        name=alg[\"algo_module_name\"]\n",
    "        print(f\"{i+1}.{new_recipe_path}_{name}_recipe.yaml\\n\")\n",
    "        !cat {new_recipe_path}_{name}_recipe.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca638c8",
   "metadata": {
    "id": "5ca638c8"
   },
   "source": [
    "Now, we will store the information of  datasets and algorithms information from <b>Paper_B's</b> recipe\n",
    "and only of the algorithms of this benchmark in the new recipe\n",
    "(execute this cell only  if Paper_B's recipe was used for benchmarking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U7PSnotYJIr2",
   "metadata": {
    "id": "U7PSnotYJIr2"
   },
   "source": [
    "### 2. Reproducing the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71159158",
   "metadata": {
    "id": "71159158",
    "outputId": "d37a18ec-5285-46ad-b379-2147cb9d2c98",
    "scrolled": false
   },
   "source": [
    "To reproduce the recipe created in the previous section,\n",
    "we refer to <b>Section 5 #ID 22(5)</b> where we can reproduce the run as well as extend benchmarks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ieghRWumgNY9",
   "metadata": {
    "id": "ieghRWumgNY9"
   },
   "source": [
    "# **9. EXTEND EXISTING BENCHMARK(own algorithm)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1HfN4e3PMn0",
   "metadata": {
    "id": "i1HfN4e3PMn0"
   },
   "source": [
    "To extend the existing benchmark here basically means to add  our own algorithm to the benchmark and to show the comparison results of pre-installed algorithms while also loading our own dataset.\n",
    "\n",
    "\n",
    "1. We load the datasets. To know how to do that, we can refer to  **Section \"1. Data\" and \"2. Data Selection\"**\n",
    "2. Then, load own algorithm as we will see in the next sub-section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kAAV6vAOy9-k",
   "metadata": {
    "id": "kAAV6vAOy9-k"
   },
   "source": [
    "## **9.1 Loading own Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cT1MEn7zSta",
   "metadata": {
    "id": "3cT1MEn7zSta"
   },
   "source": [
    "In this subsection 5.1, you will see **how an own semisupervised anomaly detection algorithm** can easily be used within oab to be evaluated. We will see how a class representing an algorithm can be structured and how its performance is evaluated.\n",
    "\n",
    "Of course, this is not the only way to use the functionality provided by oab. We do consider it to be the simplest way however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r7jxQhkcze5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "r7jxQhkcze5d",
    "outputId": "c7d5d85b-dee5-41cc-f24e-49da6f14594e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\r\n",
      "\r\n",
      "class RandomGuesserSemisupervised():\r\n",
      "\r\n",
      "    def fit(self, X_train):\r\n",
      "        pass\r\n",
      "      \r\n",
      "    def decision_function(self, X_test):\r\n",
      "        \"Assign a random number to each sample from the test set\"\r\n",
      "        n_samples = X_test.shape[0]\r\n",
      "        return np.random.randn(n_samples)\r\n"
     ]
    }
   ],
   "source": [
    "#ID 35(9)\n",
    "# used imports from #ID 2,#ID 3,#ID 8,#ID 18,#ID 21 for ths whole section\n",
    "# download example algorithm and inspect content\n",
    "import wget\n",
    "wget.download('https://raw.githubusercontent.com/jandeller/test/main/RandomGuesserSemisupervised.py', \"RandomGuesserSemisupervised.py\")\n",
    "!cat RandomGuesserSemisupervised.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enGFFsQpRJ2Y",
   "metadata": {
    "id": "enGFFsQpRJ2Y"
   },
   "source": [
    "The sample `RandomGuesser` algorithm shown here is - as the name suggests - a random guesser, i.e., it assigns random anomaly scores to the samples.\n",
    "\n",
    "An algorithm used for semisupervised anomaly detection needs to specify a `fit(X_train)` method for training and a `decision_function(X_test)` method for inference that returns an anomaly score per data point in the test set.\n",
    "\n",
    "It is of course possible to rename the method and field, use a method for accessing the anomaly scores, etc. Note that if this is done, the following code has to be changed accordingly. Adhering to the conventions described above (`fit(X_train)` and `decision_function(X_test)`) allows you to use the same interface as algorithms from [`PyOD`](https://pyod.readthedocs.io/en/latest/) as shown when [comparing algorithms using `oab`](https://colab.research.google.com/drive/1aV_itaYCJgzdZ1lQ7SUyHQ7z01xSPxDN?usp=sharing#scrollTo=QnAfCGTGL7xv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y4FYSMtIzsqZ",
   "metadata": {
    "id": "Y4FYSMtIzsqZ",
    "outputId": "1dfc5941-d181-4f31-cc5b-1ed04511e8c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myImageDataset\n",
      "mnist\n"
     ]
    }
   ],
   "source": [
    "#ID 36(9)\n",
    "# used imports from #ID 2(1),#ID 18(5)\n",
    "#used sampling parameters from #ID 15(4)\n",
    "#used benchmarking_datasets from #ID 16(4)\n",
    "\n",
    "# and import the RandomGuesser\n",
    "from RandomGuesserSemisupervised import RandomGuesserSemisupervised\n",
    "\n",
    "# looking at all the benchmarking datasets again\n",
    "for dataset_name in benchmarking_datasets:\n",
    "    print(dataset_name)\n",
    "    #print(benchmarking_datasets[dataset_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RU5K81TC0PmQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RU5K81TC0PmQ",
    "outputId": "e0c6a548-76be-46f2-f4be-4696efa51fe2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Evaluation on dataset mnist with normal labels [0] and anomaly labels [1, 2, 3, 4, 5, 6, 7, 8, 9].\n",
      "Total of 1 datasets. Per dataset:\n",
      "4832 training instances, 4142 test instances, training contamination rate 0.0, test contamination rate 0.5.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.501 \t 0.000 \t\t roc_auc\n",
      "0.503 \t 0.000 \t\t average_precision\n",
      "0.006 \t 0.000 \t\t adjusted_average_precision\n",
      "\n",
      "\n",
      ". Evaluation on dataset mnist with normal labels [0] and anomaly labels [1, 2, 3, 4, 5, 6, 7, 8, 9].\n",
      "Total of 1 datasets. Per dataset:\n",
      "4832 training instances, 4142 test instances, training contamination rate 0.0, test contamination rate 0.5.\n",
      "Mean \t Std_dev \t Metric\n",
      "0.501 \t 0.000 \t\t roc_auc\n",
      "0.503 \t 0.000 \t\t average_precision\n",
      "0.006 \t 0.000 \t\t adjusted_average_precision\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ID 37(9)\n",
    "\n",
    "#  A comparison object is created for comparing the evaluations of different Algo-Dataset combinations\n",
    "co1 = ComparisonObject()\n",
    "\n",
    "for dataset in benchmarking_datasets:\n",
    "  # evaluate the random guesser\n",
    "  eval_obj1 = EvaluationObject(algorithm_name=\"RandomGuesser\")\n",
    "  for (X_train, X_test, y_test), settings in benchmarking_datasets[dataset_name].sample_multiple_with_training_split(training_split=training_split, \n",
    "                                                                  max_contamination_rate=max_contamination_rate, \n",
    "                                                                  n_steps=n_steps):\n",
    "      print(\".\", end=\" \") # update to see progress\n",
    "      rg = RandomGuesserSemisupervised()\n",
    "      rg.fit(X_train) # data is fitted to RandomGuesser\n",
    "      pred = rg.decision_function(X_test) # and decision_scores_ is accessed\n",
    "      eval_obj1.add(y_test, pred, settings)\n",
    "\n",
    "  eval_desc = eval_obj1.evaluate()\n",
    "  # added to comparison object\n",
    "  co1.add_evaluation(eval_desc)\n",
    "  print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CHYWyeKKxs9M",
   "metadata": {
    "id": "CHYWyeKKxs9M"
   },
   "source": [
    "As in the above code, We store the evaluations of our own algorithm in evaluation object which is then added to comparison object.Similarly, we can create evaluation objects for other algorithms and add them to comparison object for final benchmarking  as shown in Section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k-8sW1dvTcNj",
   "metadata": {
    "id": "k-8sW1dvTcNj"
   },
   "source": [
    "Finally, we show below the benchmarking results of our algorithm as described in \"**Section 7. Show Benchmarking Results**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_RxojZg5zDhb",
   "metadata": {
    "id": "_RxojZg5zDhb",
    "outputId": "94d545b9-c74b-4aaf-db94-a0ea2ffeca4e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "                  mnist   Average\n",
      "RandomGuesser  0.500874  0.500874\n",
      "Average        0.500874       NaN\n",
      "For average_precision:\n",
      "                  mnist   Average\n",
      "RandomGuesser  0.503196  0.503196\n",
      "Average        0.503196       NaN\n",
      "For adjusted_average_precision:\n",
      "                  mnist   Average\n",
      "RandomGuesser  0.006392  0.006392\n",
      "Average        0.006392       NaN\n"
     ]
    }
   ],
   "source": [
    "#ID 38(9)\n",
    "\n",
    "# print results in easily readable format\n",
    "co1.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717c96b",
   "metadata": {
    "id": "1717c96b",
    "outputId": "a96423af-33eb-44a9-daa2-ee48332fdd34",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For roc_auc:\n",
      "                      mnist   Average\n",
      "RandomGuesser  0.501+-0.000  0.500874\n",
      "Average               0.501       NaN\n",
      "\n",
      "For average_precision:\n",
      "                      mnist   Average\n",
      "RandomGuesser  0.503+-0.000  0.503196\n",
      "Average               0.503       NaN\n",
      "\n",
      "For adjusted_average_precision:\n",
      "                      mnist   Average\n",
      "RandomGuesser  0.006+-0.000  0.006392\n",
      "Average               0.006       NaN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ID 39(9)\n",
    "# print results in easily readable format with standard deviations\n",
    "co1.print_results(include_stdevs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc535a8",
   "metadata": {
    "id": "0bc535a8"
   },
   "source": [
    "So,This was our example algorithm. Other algorithms can be used to run and extend benchmarks,  Please refer  to <b>#ID 19(5)</b>."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "100a6f8d"
   ],
   "name": "Semisupervised_Anomaly_Detection_on_Benchmark_Image_Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
